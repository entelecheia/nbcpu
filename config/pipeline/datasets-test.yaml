defaults:
  - datasets

use_task_as_initial_object: true
steps:
  - uses: load_dataset
    with:
      data_files: datasets/khmer/articles.jsonl
      path: json
      split: train
    verbose: true
  - uses: sample_dataset
    with:
      sample_size: 15
      randomize: true
      verbose: true
  - uses: tokenize_dataset
    with:
      tokenizer: nbcpu
      # batch_size: 3
      num_workers: 10
      text_col: text
      token_col: tokens
      # remove_columns: [bodyText, analyse_text]
      load_from_cache_file: false
      verbose: true
  - uses: extract_tokens
    with:
      tokenizer: nbcpu
      # batch_size: 3
      num_workers: 10
      token_col: tokens
      extracted_col: nouns
      nouns_only: true
      # remove_columns: [tokenizedText]
      load_from_cache_file: false
      verbose: true
  - uses: save_dataset_to_disk
    with:
      dataset_path: datasets/processed/khmer
    verbose: true
