defaults:
  - datasets

use_task_as_initial_object: true
steps:
  - uses: load_dataset
    with:
      data_files: datasets/raw/khmer_articles.parquet
      path: parquet
      split: train
    verbose: true
  - uses: sample_dataset
    with:
      sample_size: 15
      randomize: true
      verbose: true
  - uses: tokenize_dataset
    with:
      tokenizer: nbcpu
      # batch_size: 3
      num_workers: 10
      text_col: text
      token_col: tokens
      # remove_columns: [bodyText, analyse_text]
      load_from_cache_file: false
      verbose: true
  - uses: extract_tokens
    with:
      tokenizer: nbcpu
      # batch_size: 3
      num_workers: 10
      token_col: tokens
      extracted_col: nouns
      nouns_only: true
      # remove_columns: [tokenizedText]
      load_from_cache_file: false
      verbose: true
  - uses: dataset_to_pandas
    verbose: true
  - uses: dataframe_print_head_and_tail
    verbose: true
  # - uses: filter_and_sample_data
  #   with:
  #     queries:
  #       - "duplicate == False"
  #       - "cleaned_text.str.split().str.len() > 10"
  #     sample_size: 100
  #     sample_seed: 123
  #     output_dir: datasets/filtered/kakao_deduped
  #     sample_filename: sample.parquet
  #     train_filename: train.parquet
  #     discard_filename: discard.parquet
  #     verbose: true
