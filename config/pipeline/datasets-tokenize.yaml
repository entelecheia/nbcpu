defaults:
  - datasets

use_task_as_initial_object: true
steps:
  - uses: load_dataset
    with:
      data_files: datasets/raw/khmer_articles.parquet
      path: parquet
      split: train
    verbose: true
  # - uses: sample_dataset
  #   with:
  #     sample_size: 15
  #     randomize: true
  #     verbose: true
  - uses: tokenize_dataset
    with:
      tokenizer: nbcpu
      # batch_size: 3
      num_workers: 50
      text_col: text
      token_col: tokenized
      # remove_columns: [bodyText, analyse_text]
      load_from_cache_file: false
      verbose: true
  - uses: extract_tokens
    with:
      tokenizer: nbcpu
      # batch_size: 3
      num_workers: 50
      token_col: tokenized
      extracted_col: tokens
      # nouns_only: true
      # remove_columns: [tokenizedText]
      load_from_cache_file: false
      verbose: true
  - uses: dataset_to_pandas
    verbose: true
  # - uses: dataframe_print_head_and_tail
  #   with:
  #     columns: [tokens]
  #     verbose: true
  #   verbose: true
  # - uses: filter_data_by_queries
  #   with:
  #     queries:
  #       - "tokens.str.len() > 50"
  #     verbose: true
  - uses: dataframe_eval_columns
    with:
      expressions:
        id: "url.str.split('/').str[3]"
      verbose: true
  - uses: filter_and_sample_data
    with:
      queries:
        - "tokens.str.len() > 50"
      output_dir: datasets/processed/khmer_tokenized
      train_filename: train.parquet
      discard_filename: discard.parquet
      verbose: true
  - uses: dataframe_print_head_and_tail
    with:
      columns: [tokens, id]
      verbose: true
    verbose: true
  # - uses: save_dataframes
  #   with:
  #     data_file: datasets/processed/khmer_articles_tokenized.parquet
