{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling with Prior\n",
    "\n",
    "In the second stage of the analysis, the research employs a topic modeling approach with prior information to refine the topics pertinent to the study of central bank policy uncertainty in Cambodia's highly dollarized economy. The topic modeling with prior represents a sophisticated approach to distilling relevant information from a corpus of text data. By incorporating prior knowledge, the model is tailored to capture the nuances of central bank policy uncertainty in the specific context of Cambodia's economy. This method ensures that the derived topics are aligned with the research objectives, providing a robust foundation for subsequent analysis and interpretation.\n",
    "\n",
    "## Prior Information\n",
    "\n",
    "The prior information is set to guide the topic modeling towards specific themes relevant to central bank policy uncertainty. The prior consists of two main groups:\n",
    "\n",
    "- **Group 0**: Focuses on general economic indicators, including terms like 'price', 'inflation', 'growth', and 'economy'.\n",
    "- **Group 1**: Concentrates on central banking aspects, with terms such as 'nbc', 'central_bank', 'national_bank', and 'national_bank_cambodia'.\n",
    "\n",
    "## Configuration\n",
    "\n",
    "The configuration of the topic modeling with prior is as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Command Line Interface for HyFI ##\n",
      "{'about': {'authors': 'Young Joon Lee <entelecheia@hotmail.com>',\n",
      "           'description': 'Quantifying Central Bank Policy Uncertainty in a '\n",
      "                          'Highly Dollarized Economy: A Topic Modeling '\n",
      "                          'Approach',\n",
      "           'homepage': 'https://nbcpu.entelecheia.ai',\n",
      "           'license': 'MIT',\n",
      "           'name': 'Measuring Central Bank Policy Uncertainty'},\n",
      " 'debug_mode': False,\n",
      " 'dryrun': False,\n",
      " 'hydra_log_dir': '/home/yjlee/.hyfi/logs/hydra',\n",
      " 'ignore_warnings': True,\n",
      " 'logging_level': 'WARNING',\n",
      " 'model': {'_config_group_': '/model',\n",
      "           '_config_name_': 'lda',\n",
      "           '_target_': 'thematos.models.lda.LdaModel',\n",
      "           'autosave': True,\n",
      "           'batch': {'_config_group_': '/batch',\n",
      "                     '_config_name_': '__init__',\n",
      "                     'batch_name': 'model',\n",
      "                     'batch_num': None,\n",
      "                     'batch_num_auto': False,\n",
      "                     'batch_root': 'workspace/topic',\n",
      "                     'config_dirname': 'configs',\n",
      "                     'config_json': 'config.json',\n",
      "                     'config_yaml': 'config.yaml',\n",
      "                     'device': 'cpu',\n",
      "                     'num_devices': 1,\n",
      "                     'num_workers': 1,\n",
      "                     'output_extention': None,\n",
      "                     'output_suffix': None,\n",
      "                     'random_seed': False,\n",
      "                     'resume_latest': False,\n",
      "                     'resume_run': False,\n",
      "                     'seed': -1,\n",
      "                     'verbose': True},\n",
      "           'batch_name': 'model',\n",
      "           'coherence_metric_list': ['u_mass', 'c_uci', 'c_npmi', 'c_v'],\n",
      "           'corpus': {'_config_group_': '/dataset',\n",
      "                      '_config_name_': 'topic_corpus',\n",
      "                      '_target_': 'thematos.datasets.corpus.Corpus',\n",
      "                      'batch': {'_config_group_': '/batch',\n",
      "                                '_config_name_': '__init__',\n",
      "                                'batch_name': 'corpus',\n",
      "                                'batch_num': None,\n",
      "                                'batch_num_auto': False,\n",
      "                                'batch_root': 'workspace/topic',\n",
      "                                'config_dirname': 'configs',\n",
      "                                'config_json': 'config.json',\n",
      "                                'config_yaml': 'config.yaml',\n",
      "                                'device': 'cpu',\n",
      "                                'num_devices': 1,\n",
      "                                'num_workers': 1,\n",
      "                                'output_extention': None,\n",
      "                                'output_suffix': None,\n",
      "                                'random_seed': False,\n",
      "                                'resume_latest': False,\n",
      "                                'resume_run': False,\n",
      "                                'seed': -1,\n",
      "                                'verbose': True},\n",
      "                      'batch_name': 'corpus',\n",
      "                      'data_load': {'_target_': 'hyfi.utils.datasets.load.DSLoad.load_dataframe',\n",
      "                                    'columns': None,\n",
      "                                    'data_dir': None,\n",
      "                                    'data_file': 'datasets/processed/topic_noprior_filtered/train.parquet',\n",
      "                                    'filetype': None,\n",
      "                                    'index_col': None,\n",
      "                                    'verbose': False},\n",
      "                      'id_col': 'id',\n",
      "                      'module': None,\n",
      "                      'ngramize': True,\n",
      "                      'ngrams': {'_config_group_': '/ngrams',\n",
      "                                 '_config_name_': 'tp_ngrams',\n",
      "                                 '_target_': 'thematos.datasets.ngrams.NgramConfig',\n",
      "                                 'delimiter': '_',\n",
      "                                 'max_cand': 5000,\n",
      "                                 'max_len': 3,\n",
      "                                 'min_cf': 20,\n",
      "                                 'min_df': 10,\n",
      "                                 'min_score': 0.5,\n",
      "                                 'normalized': True,\n",
      "                                 'workers': 0},\n",
      "                      'path': {'_config_name_': '__batch__',\n",
      "                               'batch_name': 'corpus',\n",
      "                               'task_name': 'topic',\n",
      "                               'task_root': 'workspace'},\n",
      "                      'pipelines': [],\n",
      "                      'stopwords': {'_config_group_': '/stopwords',\n",
      "                                    '_config_name_': '__init__',\n",
      "                                    '_target_': 'lexikanon.stopwords.Stopwords',\n",
      "                                    'lowercase': True,\n",
      "                                    'name': 'stopwords',\n",
      "                                    'nltk_stopwords_lang': None,\n",
      "                                    'stopwords_fn': None,\n",
      "                                    'stopwords_list': None,\n",
      "                                    'stopwords_path': '/home/yjlee/.hyfi/logs/hydra/hyfi/2023-08-15/2023-08-15_18-29-12/tests/assets/stopwords/nbcpu-topic.txt',\n",
      "                                    'verbose': True},\n",
      "                      'task_name': 'topic',\n",
      "                      'task_root': 'workspace',\n",
      "                      'text_col': 'adjnouns',\n",
      "                      'timestamp_col': 'time',\n",
      "                      'verbose': True,\n",
      "                      'version': '0.0.0'},\n",
      "           'eval_coherence': True,\n",
      "           'model_args': {'_config_group_': '/model/config',\n",
      "                          '_config_name_': 'lda',\n",
      "                          '_target_': 'thematos.models.config.LdaConfig',\n",
      "                          'alpha': 0.1,\n",
      "                          'eta': 0.01,\n",
      "                          'k': 10,\n",
      "                          'min_cf': 10,\n",
      "                          'min_df': 10,\n",
      "                          'rm_top': 0,\n",
      "                          'tw': 1},\n",
      "           'model_type': 'LDA',\n",
      "           'module': None,\n",
      "           'path': {'_config_name_': '__batch__',\n",
      "                    'batch_name': 'model',\n",
      "                    'task_name': 'topic',\n",
      "                    'task_root': 'workspace'},\n",
      "           'pipelines': [],\n",
      "           'save_full': True,\n",
      "           'set_wordprior': True,\n",
      "           'task_name': 'topic',\n",
      "           'task_root': 'workspace',\n",
      "           'train_args': {'_config_group_': '/model/train',\n",
      "                          '_config_name_': 'topic',\n",
      "                          '_target_': 'thematos.models.config.TrainConfig',\n",
      "                          'burn_in': 0,\n",
      "                          'interval': 10,\n",
      "                          'iterations': 100},\n",
      "           'train_summary_args': {'_config_group_': '/model/summary',\n",
      "                                  '_config_name_': 'topic_train',\n",
      "                                  '_target_': 'thematos.models.config.TrainSummaryConfig',\n",
      "                                  'flush': False,\n",
      "                                  'initial_hp': True,\n",
      "                                  'params': True,\n",
      "                                  'topic_word_top_n': 10},\n",
      "           'verbose': True,\n",
      "           'version': '0.0.0',\n",
      "           'wc_args': {'_config_group_': '/model/plot',\n",
      "                       '_config_name_': 'wordcloud',\n",
      "                       '_target_': 'thematos.models.config.WordcloudConfig',\n",
      "                       'dpi': 300,\n",
      "                       'figsize': None,\n",
      "                       'fontpath': None,\n",
      "                       'height_multiple': 2,\n",
      "                       'make_collage': True,\n",
      "                       'mask_dir': None,\n",
      "                       'num_cols': 5,\n",
      "                       'num_images_per_page': 20,\n",
      "                       'num_rows': None,\n",
      "                       'output_file_format': 'wordcloud_p{page_num:02d}.png',\n",
      "                       'save': True,\n",
      "                       'title_color': 'green',\n",
      "                       'title_fontsize': 14,\n",
      "                       'titles': None,\n",
      "                       'top_n': 500,\n",
      "                       'wc': {'_config_group_': '/plot',\n",
      "                              '_config_name_': 'wordcloud',\n",
      "                              '_target_': 'thematos.plots.wordcloud.WordCloud',\n",
      "                              'background_color': 'black',\n",
      "                              'collocation_threshold': 30,\n",
      "                              'collocations': True,\n",
      "                              'color_func': None,\n",
      "                              'colormap': 'PuBu',\n",
      "                              'contour_color': 'steelblue',\n",
      "                              'contour_width': 0,\n",
      "                              'font_path': None,\n",
      "                              'font_step': 1,\n",
      "                              'height': 200,\n",
      "                              'include_numbers': False,\n",
      "                              'mask': None,\n",
      "                              'max_font_size': None,\n",
      "                              'max_words': 200,\n",
      "                              'min_font_size': 4,\n",
      "                              'min_word_length': 0,\n",
      "                              'mode': 'RGB',\n",
      "                              'normalize_plurals': True,\n",
      "                              'prefer_horizontal': 0.9,\n",
      "                              'regexp': None,\n",
      "                              'relative_scaling': 'auto',\n",
      "                              'repeat': False,\n",
      "                              'scale': 1,\n",
      "                              'stopwords': None,\n",
      "                              'width': 400},\n",
      "                       'width_multiple': 4},\n",
      "           'wordprior': {'_config_group_': '/words',\n",
      "                         '_config_name_': 'wordprior',\n",
      "                         '_target_': 'thematos.models.prior.WordPrior',\n",
      "                         'data_file': '/home/yjlee/.hyfi/logs/hydra/hyfi/2023-08-15/2023-08-15_18-29-12/tests/assets/words/word_prior.yaml',\n",
      "                         'lowercase': True,\n",
      "                         'max_prior_weight': 1.0,\n",
      "                         'min_prior_weight': 0.01,\n",
      "                         'prior_data': None,\n",
      "                         'verbose': True}},\n",
      " 'noop': 1,\n",
      " 'resolve': True,\n",
      " 'verbose': False,\n",
      " 'version': '0.14.0'}\n",
      "\n",
      "Dryrun is enabled, not running the HyFI config\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!nbcpu +model=nbcpu-topic_prior noop=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Workflow\n",
    "\n",
    "The entire workflow can be executed using the following command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[36m2023-08-15 18:39:58,536\u001b[0m][\u001b[34mhyfi.joblib.joblib\u001b[0m][\u001b[32mINFO\u001b[0m] - initialized batcher with <hyfi.joblib.batch.batcher.Batcher object at 0x7f15a43f6b80>\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:39:58,536\u001b[0m][\u001b[34mhyfi.main.config\u001b[0m][\u001b[32mINFO\u001b[0m] - HyFi project [nbcpu] initialized\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:39:58,736\u001b[0m][\u001b[34mhyfi.main.main\u001b[0m][\u001b[32mINFO\u001b[0m] - The HyFI config is not instantiatable, running HyFI task with the config\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:39:59,574\u001b[0m][\u001b[34mhyfi.joblib.joblib\u001b[0m][\u001b[32mINFO\u001b[0m] - initialized batcher with <hyfi.joblib.batch.batcher.Batcher object at 0x7f15785c1220>\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:40:00,715\u001b[0m][\u001b[34mhyfi.task.batch\u001b[0m][\u001b[32mINFO\u001b[0m] - Initalized batch: corpus(1) in /home/yjlee/workspace/projects/nbcpu/workspace/topic/corpus\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:40:02,146\u001b[0m][\u001b[34mhyfi.task.batch\u001b[0m][\u001b[32mINFO\u001b[0m] - Initalized batch: corpus(1) in /home/yjlee/workspace/projects/nbcpu/workspace/topic/corpus\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:40:02,147\u001b[0m][\u001b[34mhyfi.task.batch\u001b[0m][\u001b[32mINFO\u001b[0m] - Initalized batch: model(0) in /home/yjlee/workspace/projects/nbcpu/workspace/topic/model\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:40:02,847\u001b[0m][\u001b[34mhyfi.task.batch\u001b[0m][\u001b[32mINFO\u001b[0m] - Initalized batch: corpus(1) in /home/yjlee/workspace/projects/nbcpu/workspace/topic/corpus\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:40:04,591\u001b[0m][\u001b[34mhyfi.batch.batch\u001b[0m][\u001b[32mINFO\u001b[0m] - Setting seed to 1440953704\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:40:04,591\u001b[0m][\u001b[34mhyfi.batch.batch\u001b[0m][\u001b[32mINFO\u001b[0m] - Init batch - Batch name: model, Batch num: 0\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:40:05,390\u001b[0m][\u001b[34mhyfi.task.batch\u001b[0m][\u001b[32mINFO\u001b[0m] - Initalized batch: corpus(2) in /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_prior/corpus\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:40:05,391\u001b[0m][\u001b[34mhyfi.batch.batch\u001b[0m][\u001b[32mINFO\u001b[0m] - Init batch - Batch name: model, Batch num: 5\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:40:05,391\u001b[0m][\u001b[34mhyfi.task.batch\u001b[0m][\u001b[32mINFO\u001b[0m] - Initalized batch: model(5) in /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_prior/model\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:40:06,180\u001b[0m][\u001b[34mhyfi.task.batch\u001b[0m][\u001b[32mINFO\u001b[0m] - Initalized batch: corpus(1) in /home/yjlee/workspace/projects/nbcpu/workspace/topic/corpus\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:40:06,181\u001b[0m][\u001b[34mhyfi.task.batch\u001b[0m][\u001b[32mINFO\u001b[0m] - Initalized batch: runner(2) in /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_prior/runner\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:40:06,181\u001b[0m][\u001b[34mhyfi.workflow.workflow\u001b[0m][\u001b[32mINFO\u001b[0m] - Running task [nbcpu-topic_prior] with [run={} verbose=False uses='nbcpu-topic_prior']\u001b[0m\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s][\u001b[36m2023-08-15 18:40:06,185\u001b[0m][\u001b[34mthematos.models.prior\u001b[0m][\u001b[32mINFO\u001b[0m] - Loaded 2 words from /home/yjlee/workspace/projects/nbcpu/tests/assets/words/word_prior.yaml\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:40:06,185\u001b[0m][\u001b[34mthematos.models.prior\u001b[0m][\u001b[32mINFO\u001b[0m] - Loaded 2 priors\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:40:06,183\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - Set word prior with <WordPrior 2 priors>.\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:40:06,185\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - Set words ['price', 'growth', 'inflation', 'economy'] to topic #0 as prior.\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:40:06,185\u001b[0m][\u001b[34mthematos.datasets.corpus\u001b[0m][\u001b[32mINFO\u001b[0m] - Loading corpus...\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:40:06,185\u001b[0m][\u001b[34mthematos.datasets.corpus\u001b[0m][\u001b[32mINFO\u001b[0m] - Processing documents in the column 'adjnouns'...\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:40:34,392\u001b[0m][\u001b[34mthematos.datasets.corpus\u001b[0m][\u001b[32mINFO\u001b[0m] - Total 27594 documents are loaded.\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:40:46,145\u001b[0m][\u001b[34mhyfi.utils.datasets.save\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving dataframe to /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_prior/corpus/corpus_doc_ids.parquet\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:40:46,168\u001b[0m][\u001b[34mhyfi.composer.config\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving config to /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_prior/corpus/configs/corpus(2)_config.json\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:40:46,168\u001b[0m][\u001b[34mhyfi.composer.config\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving config to /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_prior/corpus/configs/corpus(2)_config.yaml\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:40:46,788\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - Set words ['national_bank_cambodia', 'central_bank', 'nbc', 'national_bank'] to topic #1 as prior.\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:40:47,560\u001b[0m][\u001b[34mthematos.models.lda\u001b[0m][\u001b[32mINFO\u001b[0m] - Number of docs: 27594\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:40:47,560\u001b[0m][\u001b[34mthematos.models.lda\u001b[0m][\u001b[32mINFO\u001b[0m] - Vocab size: 18158\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:40:47,561\u001b[0m][\u001b[34mthematos.models.lda\u001b[0m][\u001b[32mINFO\u001b[0m] - Number of words: 4810963\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:40:47,561\u001b[0m][\u001b[34mthematos.models.lda\u001b[0m][\u001b[32mINFO\u001b[0m] - Removed top words: []\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:40:47,561\u001b[0m][\u001b[34mthematos.models.lda\u001b[0m][\u001b[32mINFO\u001b[0m] - Training model by iterating over the corpus 100 times, 10 iterations at a time with 0 workers\u001b[0m\n",
      "\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A[\u001b[36m2023-08-15 18:40:51,040\u001b[0m][\u001b[34mthematos.models.lda\u001b[0m][\u001b[32mINFO\u001b[0m] - Iteration: 0\tLog-likelihood: -9.145340999720068\u001b[0m\n",
      "\n",
      " 10%|████▍                                       | 1/10 [00:03<00:31,  3.49s/it]\u001b[A[\u001b[36m2023-08-15 18:40:54,537\u001b[0m][\u001b[34mthematos.models.lda\u001b[0m][\u001b[32mINFO\u001b[0m] - Iteration: 10\tLog-likelihood: -8.888749004161726\u001b[0m\n",
      "\n",
      " 20%|████████▊                                   | 2/10 [00:06<00:27,  3.49s/it]\u001b[A[\u001b[36m2023-08-15 18:40:58,485\u001b[0m][\u001b[34mthematos.models.lda\u001b[0m][\u001b[32mINFO\u001b[0m] - Iteration: 20\tLog-likelihood: -8.803375071366277\u001b[0m\n",
      "\n",
      " 30%|█████████████▏                              | 3/10 [00:10<00:26,  3.72s/it]\u001b[A[\u001b[36m2023-08-15 18:41:02,298\u001b[0m][\u001b[34mthematos.models.lda\u001b[0m][\u001b[32mINFO\u001b[0m] - Iteration: 30\tLog-likelihood: -8.749849464728818\u001b[0m\n",
      "\n",
      " 40%|█████████████████▌                          | 4/10 [00:14<00:22,  3.74s/it]\u001b[A[\u001b[36m2023-08-15 18:41:06,123\u001b[0m][\u001b[34mthematos.models.lda\u001b[0m][\u001b[32mINFO\u001b[0m] - Iteration: 40\tLog-likelihood: -8.714705246341687\u001b[0m\n",
      "\n",
      " 50%|██████████████████████                      | 5/10 [00:18<00:18,  3.78s/it]\u001b[A[\u001b[36m2023-08-15 18:41:09,859\u001b[0m][\u001b[34mthematos.models.lda\u001b[0m][\u001b[32mINFO\u001b[0m] - Iteration: 50\tLog-likelihood: -8.690089545139084\u001b[0m\n",
      "\n",
      " 60%|██████████████████████████▍                 | 6/10 [00:22<00:15,  3.76s/it]\u001b[A[\u001b[36m2023-08-15 18:41:13,547\u001b[0m][\u001b[34mthematos.models.lda\u001b[0m][\u001b[32mINFO\u001b[0m] - Iteration: 60\tLog-likelihood: -8.668976701892165\u001b[0m\n",
      "\n",
      " 70%|██████████████████████████████▊             | 7/10 [00:25<00:11,  3.73s/it]\u001b[A[\u001b[36m2023-08-15 18:41:17,288\u001b[0m][\u001b[34mthematos.models.lda\u001b[0m][\u001b[32mINFO\u001b[0m] - Iteration: 70\tLog-likelihood: -8.65117961727829\u001b[0m\n",
      "\n",
      " 80%|███████████████████████████████████▏        | 8/10 [00:29<00:07,  3.74s/it]\u001b[A[\u001b[36m2023-08-15 18:41:20,956\u001b[0m][\u001b[34mthematos.models.lda\u001b[0m][\u001b[32mINFO\u001b[0m] - Iteration: 80\tLog-likelihood: -8.637118155842469\u001b[0m\n",
      "\n",
      " 90%|███████████████████████████████████████▌    | 9/10 [00:33<00:03,  3.71s/it]\u001b[A[\u001b[36m2023-08-15 18:41:24,630\u001b[0m][\u001b[34mthematos.models.lda\u001b[0m][\u001b[32mINFO\u001b[0m] - Iteration: 90\tLog-likelihood: -8.62476473618058\u001b[0m\n",
      "\n",
      "100%|███████████████████████████████████████████| 10/10 [00:37<00:00,  3.71s/it]\u001b[A\n",
      "<Basic Info>\n",
      "| LDAModel (current version: 0.12.5)\n",
      "| 27594 docs, 4810963 words\n",
      "| Total Vocabs: 126469, Used Vocabs: 18158\n",
      "| Entropy of words: 7.96401\n",
      "| Entropy of term-weighted words: 8.76557\n",
      "| Removed Vocabs: <NA>\n",
      "|\n",
      "<Training Info>\n",
      "| Iterations: 100, Burn-in steps: 0\n",
      "| Optimization Interval: 10\n",
      "| Log-likelihood per word: -8.62476\n",
      "|\n",
      "<Initial Parameters>\n",
      "| tw: TermWeight.IDF\n",
      "| min_cf: 10 (minimum collection frequency of words)\n",
      "| min_df: 10 (minimum document frequency of words)\n",
      "| rm_top: 0 (the number of top words to be removed)\n",
      "| k: 10 (the number of topics between 1 ~ 32767)\n",
      "| alpha: [0.1] (hyperparameter of Dirichlet distribution for document-topic, given as a single `float` in case of symmetric prior and as a list with length `k` of `float` in case of asymmetric prior.)\n",
      "| eta: 0.01 (hyperparameter of Dirichlet distribution for topic-word)\n",
      "| seed: 1440953704 (random seed)\n",
      "| trained in version 0.12.5\n",
      "|\n",
      "<Parameters>\n",
      "| alpha (Dirichlet prior on the per-document topic distributions)\n",
      "|  [0.10728438 0.06657154 0.07608083 0.06004212 0.06778088 0.11082149\n",
      "|   0.06340487 0.05877907 0.05908309 0.05704119]\n",
      "| eta (Dirichlet prior on the per-topic word distribution)\n",
      "|  0.01\n",
      "|\n",
      "<Topics>\n",
      "| #0 (742782) : price market growth inflation rate economy oil share dollar global stock low index investor trade high export bank debt trading\n",
      "| #1 (619991) : bank digital customer loan payment banking nbc service financial business insurance company cambodia riel mobile transaction technology smes branch financial_institution\n",
      "| #2 (403293) : energy india power climate world green climate_change emission technology coal solar global company way malaysia system data renewable_energy human research\n",
      "| #3 (313718) : tourism tourist chinese airport city hotel visitor flight cambodia passenger province siem_reap angkor travel destination airline temple sihanoukville phnom_penh cambodian\n",
      "| #4 (421016) : worker factory project export port garment road construction company cambodia investment logistics labour ministry union sector electricity japanese vehicle industry\n",
      "| #5 (772618) : cambodia tax development asean cooperation economic trade policy government investment woman minister law agreement cambodian social meeting sector sustainable member\n",
      "| #6 (401326) : election party political military myanmar democracy leader state human_right government opposition protest bangladesh former khmer_rouge right pakistan politics power india\n",
      "| #7 (386550) : rice farmer water land project agriculture river food fish tonne property province area forest agricultural community crop village price fishery\n",
      "| #8 (395547) : police court victim commune party drug cnrp prison cpp district law money case election judge provincial suspect crime phnom_penh complaint\n",
      "| #9 (354122) : health covid-19 vaccine case virus child pandemic vaccination infection huawei disease medical omicron outbreak community patient hospital covid quarantine death\n",
      "|\n",
      "\n",
      "[\u001b[36m2023-08-15 18:41:25,291\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - ==== Coherence : u_mass ====\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:41:25,291\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - Average: -1.6402675149726402\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:41:25,292\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - Per Topic: [-1.425266578567007, -1.0729033256163611, -1.9737517568414404, -1.6950357738440491, -1.4372304993750558, -1.0894857078690348, -1.3641567969557398, -1.9651203444009737, -2.0280244511223575, -2.3516999151343816]\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:41:25,911\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - ==== Coherence : c_uci ====\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:41:25,911\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - Average: 0.7173903977549194\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:41:25,911\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - Per Topic: [0.631636197256756, 1.0372400239549309, 1.0279241224565632, 1.1587594141923576, 0.4124652574389333, 0.3280867476153075, 0.9620709742174812, 0.8812465779074348, 0.9351462283826499, -0.20067156587322113]\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:41:26,612\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - ==== Coherence : c_npmi ====\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:41:26,613\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - Average: 0.10520817835902437\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:41:26,613\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - Per Topic: [0.09169010689655911, 0.1513512764679383, 0.11547400424216366, 0.1390206534657676, 0.06185781111793033, 0.05339414794065087, 0.11865170803328073, 0.10947131563054402, 0.12918262344786416, 0.08198813634754491]\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:41:30,242\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - ==== Coherence : c_v ====\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:41:30,242\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - Average: 0.7093467557430267\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:41:30,242\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - Per Topic: [0.7229848951101303, 0.8240539312362671, 0.6891917288303375, 0.7377653867006302, 0.5392432540655137, 0.5775677382946014, 0.7515179306268692, 0.7305361032485962, 0.787589567899704, 0.7330170214176178]\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:41:30,303\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - Model saved to /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_prior/model/models/LDA_model(5)_k(10).mdl\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:41:30,361\u001b[0m][\u001b[34mhyfi.utils.datasets.save\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving dataframe to /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_prior/model/outputs/LDA_model(5)_k(10)-ll_per_word.csv\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:41:30,363\u001b[0m][\u001b[34mhyfi.utils.datasets.save\u001b[0m][\u001b[32mINFO\u001b[0m] -  >> elapsed time to save data: 0:00:00.001810\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:41:30,766\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - Log-likelihood per word plot saved to /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_prior/model/outputs/LDA_model(5)_k(10)-ll_per_word.png\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:41:30,824\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - ==== Document-Topic Distributions ====\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:41:30,825\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] -           id    topic0    topic1  ...    topic7    topic8    topic9\n",
      "27589  48239  0.833621  0.000136  ...  0.000120  0.083921  0.000116\n",
      "27590  48216  0.086417  0.000143  ...  0.133420  0.000127  0.000123\n",
      "27591  48132  0.000117  0.000073  ...  0.000064  0.319019  0.000062\n",
      "27592  48115  0.070176  0.000184  ...  0.000163  0.000164  0.000158\n",
      "27593  48085  0.000108  0.169788  ...  0.000059  0.000060  0.000058\n",
      "\n",
      "[5 rows x 11 columns]\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:41:30,842\u001b[0m][\u001b[34mhyfi.utils.datasets.save\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving dataframe to /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_prior/model/outputs/LDA_model(5)_k(10)-doc_topic_dists.parquet\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:41:30,933\u001b[0m][\u001b[34mhyfi.utils.datasets.save\u001b[0m][\u001b[32mINFO\u001b[0m] -  >> elapsed time to save data: 0:00:00.090841\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:41:30,933\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - ==== Topic-Word Distributions ====\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:41:30,938\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] -        cambodia  government  ...  rcep_cambodia-china_free  asia_pacific_region\n",
      "5  7.159818e-03    0.003812  ...              4.491441e-09         4.491441e-09\n",
      "6  6.598678e-09    0.002988  ...              6.515646e-09         6.515646e-09\n",
      "7  1.485342e-03    0.001250  ...              7.418155e-09         7.418155e-09\n",
      "8  2.527701e-04    0.000561  ...              6.831890e-09         6.832542e-09\n",
      "9  2.897601e-03    0.001661  ...              8.522984e-09         4.052390e-05\n",
      "\n",
      "[5 rows x 18158 columns]\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:41:31,048\u001b[0m][\u001b[34mhyfi.utils.datasets.save\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving dataframe to /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_prior/model/outputs/LDA_model(5)_k(10)-topic_term_dists.parquet\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:41:33,325\u001b[0m][\u001b[34mhyfi.utils.datasets.save\u001b[0m][\u001b[32mINFO\u001b[0m] -  >> elapsed time to save data: 0:00:02.276425\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:41:33,356\u001b[0m][\u001b[34mhyfi.utils.iolibs\u001b[0m][\u001b[32mINFO\u001b[0m] - Save the list to the file: /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_prior/model/outputs/LDA_model(5)_k(10)-used_vocab.txt, no. of words: 18158\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:41:33,367\u001b[0m][\u001b[34mhyfi.utils.iolibs\u001b[0m][\u001b[32mINFO\u001b[0m] - Save the list to the file: /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_prior/model/outputs/LDA_model(5)_k(10)-topic_top_words.txt, no. of words: 410\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:41:33,368\u001b[0m][\u001b[34mhyfi.utils.datasets.save\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving dataframe to /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_prior/model/outputs/LDA_model(5)_k(10)-topic_top_words_dists.csv\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:41:33,370\u001b[0m][\u001b[34mhyfi.utils.datasets.save\u001b[0m][\u001b[32mINFO\u001b[0m] -  >> elapsed time to save data: 0:00:00.001600\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:41:40,187\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - Making wordcloud collage with titles: ['Topic 0', 'Topic 1', 'Topic 2', 'Topic 3', 'Topic 4', 'Topic 5', 'Topic 6', 'Topic 7', 'Topic 8', 'Topic 9']\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:41:40,187\u001b[0m][\u001b[34mhyfi.graphics.collage\u001b[0m][\u001b[32mINFO\u001b[0m] - Making page 1/1 with 10 images\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:41:40,187\u001b[0m][\u001b[34mhyfi.graphics.collage\u001b[0m][\u001b[32mINFO\u001b[0m] - Page titles: ['Topic 0', 'Topic 1', 'Topic 2', 'Topic 3', 'Topic 4', 'Topic 5', 'Topic 6', 'Topic 7', 'Topic 8', 'Topic 9']\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:41:40,187\u001b[0m][\u001b[34mhyfi.graphics.collage\u001b[0m][\u001b[32mINFO\u001b[0m] - Page output file: /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_prior/model/outputs/wordcloud_collage/LDA_model(5)_k(10)_wordcloud_00.png\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:41:43,448\u001b[0m][\u001b[34mhyfi.graphics.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - Saved subplots to /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_prior/model/outputs/wordcloud_collage/LDA_model(5)_k(10)_wordcloud_00.png\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:41:43,449\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[33mWARNING\u001b[0m] - pyLDAvis is not installed. Please install it to save LDAvis.\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:41:43,465\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - Model summary saved to /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_prior/model/outputs/model-summary.jsonl\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:41:43,465\u001b[0m][\u001b[34mhyfi.composer.config\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving config to /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_prior/model/configs/model(5)_config.json\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:41:43,466\u001b[0m][\u001b[34mhyfi.composer.config\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving config to /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_prior/model/configs/model(5)_config.yaml\u001b[0m\n",
      "100%|█████████████████████████████████████████████| 1/1 [01:37<00:00, 97.31s/it]\n",
      "[\u001b[36m2023-08-15 18:41:43,491\u001b[0m][\u001b[34mthematos.runners.topic\u001b[0m][\u001b[32mINFO\u001b[0m] - Saved summaries to /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_prior/runner(2)_summaries.json\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:41:43,491\u001b[0m][\u001b[34mhyfi.composer.config\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving config to /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_prior/runner/configs/runner(2)_config.json\u001b[0m\n",
      "[\u001b[36m2023-08-15 18:41:43,492\u001b[0m][\u001b[34mhyfi.composer.config\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving config to /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_prior/runner/configs/runner(2)_config.yaml\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!nbcpu +workflow=nbcpu tasks='[nbcpu-topic_prior]' mode=__info__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Results\n",
    "\n",
    "The Latent Dirichlet Allocation (LDA) model, applied to a corpus of 27,594 documents encompassing 4,810,963 words, utilized 18,158 out of 126,469 total vocabs. Configured with 10 topics and specific hyperparameters for alpha and eta, the model underwent 100 iterations without burn-in steps, with an optimization interval of 10.\n",
    "\n",
    "The resultant topics reflect a diverse spectrum of subjects, including economics, finance, politics, technology, and social matters. Through the incorporation of prior information, the model was steered towards themes pertinent to central bank policy and economic indicators. Specifically:\n",
    "\n",
    "- **Topic #0** aligns with the economic indicators' prior, emphasizing price, market growth, inflation, and global economic aspects.\n",
    "- **Topic #1** resonates with the banking sector, including central banking, mirroring the prior set for banking and financial services.\n",
    "- **Topic #7** encapsulates trade, investment, and regional cooperation facets, including ASEAN, indirectly correlating with economic policy and central banking.\n",
    "\n",
    "These topics collectively illustrate the model's adeptness in leveraging prior information to hone in on areas of interest such as central banking and economic indicators. The coherence of the topics underscores their interpretability and relevance to the overarching research focus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{numref}`fig-lda-wordcloud-prior` shows the wordcloud of the top 500 words in each topic from the LDA model with 10 topics and prior. The size of the word is proportional to the frequency of the word in the topic.\n",
    "\n",
    "```{figure} ./figs/LDA_model(0)_k(10)_wordcloud_00.png\n",
    "---\n",
    "name: fig-lda-wordcloud-prior\n",
    "---\n",
    "Wordcloud of the top 500 words in each topic from the LDA model with 10 topics and prior. The size of the word is proportional to the frequency of the word in the topic.\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research-P5qh1W2b-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
