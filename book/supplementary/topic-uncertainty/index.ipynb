{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling for Uncertainty Analysis\n",
    "\n",
    "The task of classifying uncertainty within the context of topic modeling requires a nuanced and specialized approach, diverging from conventional methods. This methodology is characterized by three distinct modifications, each contributing to the precise identification and analysis of uncertainty-related themes.\n",
    "\n",
    "### 1. Integration of Prior Knowledge\n",
    "\n",
    "The model is designed to incorporate prior knowledge, which serves as a guiding framework for identifying topics specifically related to uncertainty. The defined priors include:\n",
    "\n",
    "- **Prior 0**: Targeting terms synonymous with uncertainty, such as \"uncertain,\" \"risk,\" and \"uncertainty.\"\n",
    "- **Prior 1**: Concentrating on terms that denote enhancement or fortification, such as \"improve,\" \"strengthen,\" \"ensure,\" and \"enhance.\"\n",
    "\n",
    "These priors strategically channel the model's focus towards themes intrinsically connected to uncertainty, thereby refining the relevance and specificity of the extracted topics.\n",
    "\n",
    "### 2. Strategic Removal of Stop Words\n",
    "\n",
    "Beyond the elimination of standard stop words, the model is configured to exclude the top 100 words from each of the 10 topics identified in the previous model, totaling 896 words. This exclusion is carefully executed to preserve uncertainty-related words through manual inspection. By filtering out these common but non-contributory words, the model is enabled to concentrate on terms that are contextually significant and pertinent to the understanding of uncertainty.\n",
    "\n",
    "### 3. Fine-Tuning of Model Parameters\n",
    "\n",
    "The model's parameters are meticulously adjusted to resonate with the unique objective of classifying uncertainty. The key modifications include:\n",
    "\n",
    "- **min_cf**: Set at 500, this parameter stipulates the minimum collection frequency of words, focusing on terms with substantial corpus-wide presence.\n",
    "- **min_df**: Also set at 500, this parameter governs the minimum document frequency of words, accentuating terms that recur across diverse documents.\n",
    "\n",
    "These deliberate parameter adjustments, coupled with the strategic integration of prior knowledge and the removal of specific stop words, culminate in a bespoke approach to topic modeling. This approach is adept at capturing and classifying uncertainty-related themes, aligning seamlessly with the overarching research goals.\n",
    "\n",
    "## Configuration Details\n",
    "\n",
    "The configuration for this specialized topic modeling with uncertainty analysis is encapsulated in the following command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Command Line Interface for HyFI ##\n",
      "{'about': {'authors': 'Young Joon Lee <entelecheia@hotmail.com>',\n",
      "           'description': 'Quantifying Central Bank Policy Uncertainty in a '\n",
      "                          'Highly Dollarized Economy: A Topic Modeling '\n",
      "                          'Approach',\n",
      "           'homepage': 'https://nbcpu.entelecheia.ai',\n",
      "           'license': 'MIT',\n",
      "           'name': 'Measuring Central Bank Policy Uncertainty'},\n",
      " 'debug_mode': False,\n",
      " 'dryrun': False,\n",
      " 'hydra_log_dir': '/home/yjlee/.hyfi/logs/hydra',\n",
      " 'ignore_warnings': True,\n",
      " 'logging_level': 'WARNING',\n",
      " 'model': {'_config_group_': '/model',\n",
      "           '_config_name_': 'lda',\n",
      "           '_target_': 'thematos.models.lda.LdaModel',\n",
      "           'autosave': True,\n",
      "           'batch': {'_config_group_': '/batch',\n",
      "                     '_config_name_': '__init__',\n",
      "                     'batch_name': 'model',\n",
      "                     'batch_num': None,\n",
      "                     'batch_num_auto': False,\n",
      "                     'batch_root': 'workspace/topic',\n",
      "                     'config_dirname': 'configs',\n",
      "                     'config_json': 'config.json',\n",
      "                     'config_yaml': 'config.yaml',\n",
      "                     'device': 'cpu',\n",
      "                     'num_devices': 1,\n",
      "                     'num_workers': 1,\n",
      "                     'output_extention': None,\n",
      "                     'output_suffix': None,\n",
      "                     'random_seed': False,\n",
      "                     'resume_latest': False,\n",
      "                     'resume_run': False,\n",
      "                     'seed': -1,\n",
      "                     'verbose': True},\n",
      "           'batch_name': 'model',\n",
      "           'coherence_metric_list': ['u_mass', 'c_uci', 'c_npmi', 'c_v'],\n",
      "           'corpus': {'_config_group_': '/dataset',\n",
      "                      '_config_name_': 'topic_corpus',\n",
      "                      '_target_': 'thematos.datasets.corpus.Corpus',\n",
      "                      'batch': {'_config_group_': '/batch',\n",
      "                                '_config_name_': '__init__',\n",
      "                                'batch_name': 'corpus',\n",
      "                                'batch_num': None,\n",
      "                                'batch_num_auto': False,\n",
      "                                'batch_root': 'workspace/topic',\n",
      "                                'config_dirname': 'configs',\n",
      "                                'config_json': 'config.json',\n",
      "                                'config_yaml': 'config.yaml',\n",
      "                                'device': 'cpu',\n",
      "                                'num_devices': 1,\n",
      "                                'num_workers': 1,\n",
      "                                'output_extention': None,\n",
      "                                'output_suffix': None,\n",
      "                                'random_seed': False,\n",
      "                                'resume_latest': False,\n",
      "                                'resume_run': False,\n",
      "                                'seed': -1,\n",
      "                                'verbose': True},\n",
      "                      'batch_name': 'corpus',\n",
      "                      'data_load': {'_target_': 'hyfi.utils.datasets.load.DSLoad.load_dataframe',\n",
      "                                    'columns': None,\n",
      "                                    'data_dir': None,\n",
      "                                    'data_file': 'datasets/processed/topic_noprior_filtered/train.parquet',\n",
      "                                    'filetype': None,\n",
      "                                    'index_col': None,\n",
      "                                    'verbose': False},\n",
      "                      'id_col': 'id',\n",
      "                      'module': None,\n",
      "                      'ngramize': True,\n",
      "                      'ngrams': {'_config_group_': '/ngrams',\n",
      "                                 '_config_name_': 'tp_ngrams',\n",
      "                                 '_target_': 'thematos.datasets.ngrams.NgramConfig',\n",
      "                                 'delimiter': '_',\n",
      "                                 'max_cand': 5000,\n",
      "                                 'max_len': 3,\n",
      "                                 'min_cf': 20,\n",
      "                                 'min_df': 10,\n",
      "                                 'min_score': 0.5,\n",
      "                                 'normalized': True,\n",
      "                                 'workers': 0},\n",
      "                      'path': {'_config_name_': '__batch__',\n",
      "                               'batch_name': 'corpus',\n",
      "                               'task_name': 'topic',\n",
      "                               'task_root': 'workspace'},\n",
      "                      'pipelines': [],\n",
      "                      'stopwords': {'_config_group_': '/stopwords',\n",
      "                                    '_config_name_': '__init__',\n",
      "                                    '_target_': 'lexikanon.stopwords.Stopwords',\n",
      "                                    'lowercase': True,\n",
      "                                    'name': 'stopwords',\n",
      "                                    'nltk_stopwords_lang': None,\n",
      "                                    'stopwords_fn': None,\n",
      "                                    'stopwords_list': None,\n",
      "                                    'stopwords_path': '/home/yjlee/.hyfi/logs/hydra/hyfi/2023-08-15/2023-08-15_19-05-04/tests/assets/stopwords/nbcpu-uncertainty.txt',\n",
      "                                    'verbose': True},\n",
      "                      'task_name': 'topic',\n",
      "                      'task_root': 'workspace',\n",
      "                      'text_col': 'tokens',\n",
      "                      'timestamp_col': 'time',\n",
      "                      'verbose': True,\n",
      "                      'version': '0.0.0'},\n",
      "           'eval_coherence': True,\n",
      "           'model_args': {'_config_group_': '/model/config',\n",
      "                          '_config_name_': 'lda',\n",
      "                          '_target_': 'thematos.models.config.LdaConfig',\n",
      "                          'alpha': 0.1,\n",
      "                          'eta': 0.01,\n",
      "                          'k': 10,\n",
      "                          'min_cf': 500,\n",
      "                          'min_df': 500,\n",
      "                          'rm_top': 0,\n",
      "                          'tw': 1},\n",
      "           'model_type': 'LDA',\n",
      "           'module': None,\n",
      "           'path': {'_config_name_': '__batch__',\n",
      "                    'batch_name': 'model',\n",
      "                    'task_name': 'topic',\n",
      "                    'task_root': 'workspace'},\n",
      "           'pipelines': [],\n",
      "           'save_full': True,\n",
      "           'set_wordprior': True,\n",
      "           'task_name': 'topic',\n",
      "           'task_root': 'workspace',\n",
      "           'train_args': {'_config_group_': '/model/train',\n",
      "                          '_config_name_': 'topic',\n",
      "                          '_target_': 'thematos.models.config.TrainConfig',\n",
      "                          'burn_in': 0,\n",
      "                          'interval': 10,\n",
      "                          'iterations': 100},\n",
      "           'train_summary_args': {'_config_group_': '/model/summary',\n",
      "                                  '_config_name_': 'topic_train',\n",
      "                                  '_target_': 'thematos.models.config.TrainSummaryConfig',\n",
      "                                  'flush': False,\n",
      "                                  'initial_hp': True,\n",
      "                                  'params': True,\n",
      "                                  'topic_word_top_n': 10},\n",
      "           'verbose': True,\n",
      "           'version': '0.0.0',\n",
      "           'wc_args': {'_config_group_': '/model/plot',\n",
      "                       '_config_name_': 'wordcloud',\n",
      "                       '_target_': 'thematos.models.config.WordcloudConfig',\n",
      "                       'dpi': 300,\n",
      "                       'figsize': None,\n",
      "                       'fontpath': None,\n",
      "                       'height_multiple': 2,\n",
      "                       'make_collage': True,\n",
      "                       'mask_dir': None,\n",
      "                       'num_cols': 5,\n",
      "                       'num_images_per_page': 20,\n",
      "                       'num_rows': None,\n",
      "                       'output_file_format': 'wordcloud_p{page_num:02d}.png',\n",
      "                       'save': True,\n",
      "                       'title_color': 'green',\n",
      "                       'title_fontsize': 14,\n",
      "                       'titles': None,\n",
      "                       'top_n': 500,\n",
      "                       'wc': {'_config_group_': '/plot',\n",
      "                              '_config_name_': 'wordcloud',\n",
      "                              '_target_': 'thematos.plots.wordcloud.WordCloud',\n",
      "                              'background_color': 'black',\n",
      "                              'collocation_threshold': 30,\n",
      "                              'collocations': True,\n",
      "                              'color_func': None,\n",
      "                              'colormap': 'PuBu',\n",
      "                              'contour_color': 'steelblue',\n",
      "                              'contour_width': 0,\n",
      "                              'font_path': None,\n",
      "                              'font_step': 1,\n",
      "                              'height': 200,\n",
      "                              'include_numbers': False,\n",
      "                              'mask': None,\n",
      "                              'max_font_size': None,\n",
      "                              'max_words': 200,\n",
      "                              'min_font_size': 4,\n",
      "                              'min_word_length': 0,\n",
      "                              'mode': 'RGB',\n",
      "                              'normalize_plurals': True,\n",
      "                              'prefer_horizontal': 0.9,\n",
      "                              'regexp': None,\n",
      "                              'relative_scaling': 'auto',\n",
      "                              'repeat': False,\n",
      "                              'scale': 1,\n",
      "                              'stopwords': None,\n",
      "                              'width': 400},\n",
      "                       'width_multiple': 4},\n",
      "           'wordprior': {'_config_group_': '/words',\n",
      "                         '_config_name_': 'wordprior',\n",
      "                         '_target_': 'thematos.models.prior.WordPrior',\n",
      "                         'data_file': '/home/yjlee/.hyfi/logs/hydra/hyfi/2023-08-15/2023-08-15_19-05-04/tests/assets/words/word_prior_uncertainty.yaml',\n",
      "                         'lowercase': True,\n",
      "                         'max_prior_weight': 1.0,\n",
      "                         'min_prior_weight': 0.01,\n",
      "                         'prior_data': None,\n",
      "                         'verbose': True}},\n",
      " 'noop': 1,\n",
      " 'resolve': True,\n",
      " 'verbose': False,\n",
      " 'version': '0.15.0'}\n",
      "\n",
      "Dryrun is enabled, not running the HyFI config\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!nbcpu +model=nbcpu-topic_uncertainty noop=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Workflow\n",
    "\n",
    "The entire workflow can be executed using the following command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[36m2023-08-15 19:07:43,714\u001b[0m][\u001b[34mhyfi.joblib.joblib\u001b[0m][\u001b[32mINFO\u001b[0m] - initialized batcher with <hyfi.joblib.batch.batcher.Batcher object at 0x7f5a586f5670>\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:07:43,714\u001b[0m][\u001b[34mhyfi.main.config\u001b[0m][\u001b[32mINFO\u001b[0m] - HyFi project [nbcpu] initialized\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:07:43,912\u001b[0m][\u001b[34mhyfi.main.main\u001b[0m][\u001b[32mINFO\u001b[0m] - The HyFI config is not instantiatable, running HyFI task with the config\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:07:44,736\u001b[0m][\u001b[34mhyfi.joblib.joblib\u001b[0m][\u001b[32mINFO\u001b[0m] - initialized batcher with <hyfi.joblib.batch.batcher.Batcher object at 0x7f5a387bf250>\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:07:45,870\u001b[0m][\u001b[34mhyfi.task.batch\u001b[0m][\u001b[32mINFO\u001b[0m] - Initalized batch: corpus(1) in /home/yjlee/workspace/projects/nbcpu/workspace/topic/corpus\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:07:47,283\u001b[0m][\u001b[34mhyfi.task.batch\u001b[0m][\u001b[32mINFO\u001b[0m] - Initalized batch: corpus(1) in /home/yjlee/workspace/projects/nbcpu/workspace/topic/corpus\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:07:47,283\u001b[0m][\u001b[34mhyfi.task.batch\u001b[0m][\u001b[32mINFO\u001b[0m] - Initalized batch: model(0) in /home/yjlee/workspace/projects/nbcpu/workspace/topic/model\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:07:47,976\u001b[0m][\u001b[34mhyfi.task.batch\u001b[0m][\u001b[32mINFO\u001b[0m] - Initalized batch: corpus(1) in /home/yjlee/workspace/projects/nbcpu/workspace/topic/corpus\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:07:49,701\u001b[0m][\u001b[34mhyfi.batch.batch\u001b[0m][\u001b[32mINFO\u001b[0m] - Setting seed to 590167864\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:07:49,701\u001b[0m][\u001b[34mhyfi.batch.batch\u001b[0m][\u001b[32mINFO\u001b[0m] - Init batch - Batch name: model, Batch num: 0\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:07:50,313\u001b[0m][\u001b[34mhyfi.batch.batch\u001b[0m][\u001b[32mINFO\u001b[0m] - Setting seed to 2225682814\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:07:50,313\u001b[0m][\u001b[34mhyfi.batch.batch\u001b[0m][\u001b[32mINFO\u001b[0m] - Init batch - Batch name: corpus, Batch num: 0\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:07:50,485\u001b[0m][\u001b[34mhyfi.batch.batch\u001b[0m][\u001b[32mINFO\u001b[0m] - Init batch - Batch name: corpus, Batch num: 1\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:07:50,485\u001b[0m][\u001b[34mhyfi.task.batch\u001b[0m][\u001b[32mINFO\u001b[0m] - Initalized batch: corpus(1) in /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_uncertainty/corpus\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:07:50,485\u001b[0m][\u001b[34mhyfi.batch.batch\u001b[0m][\u001b[32mINFO\u001b[0m] - Init batch - Batch name: model, Batch num: 4\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:07:50,486\u001b[0m][\u001b[34mhyfi.task.batch\u001b[0m][\u001b[32mINFO\u001b[0m] - Initalized batch: model(4) in /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_uncertainty/model\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:07:51,274\u001b[0m][\u001b[34mhyfi.task.batch\u001b[0m][\u001b[32mINFO\u001b[0m] - Initalized batch: corpus(1) in /home/yjlee/workspace/projects/nbcpu/workspace/topic/corpus\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:07:51,274\u001b[0m][\u001b[34mhyfi.task.batch\u001b[0m][\u001b[32mINFO\u001b[0m] - Initalized batch: runner(1) in /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_uncertainty/runner\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:07:51,275\u001b[0m][\u001b[34mhyfi.workflow.workflow\u001b[0m][\u001b[32mINFO\u001b[0m] - Running task [nbcpu-topic_uncertainty] with [run={} verbose=False uses='nbcpu-topic_uncertainty']\u001b[0m\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s][\u001b[36m2023-08-15 19:07:51,278\u001b[0m][\u001b[34mthematos.models.prior\u001b[0m][\u001b[32mINFO\u001b[0m] - Loaded 2 words from /home/yjlee/workspace/projects/nbcpu/tests/assets/words/word_prior_uncertainty.yaml\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:07:51,278\u001b[0m][\u001b[34mthematos.models.prior\u001b[0m][\u001b[32mINFO\u001b[0m] - Loaded 2 priors\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:07:51,277\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - Set word prior with <WordPrior 2 priors>.\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:07:51,278\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - Set words ['uncertain', 'uncertainty', 'risk'] to topic #0 as prior.\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:07:51,279\u001b[0m][\u001b[34mthematos.datasets.corpus\u001b[0m][\u001b[32mINFO\u001b[0m] - Loading corpus...\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:07:51,279\u001b[0m][\u001b[34mthematos.datasets.corpus\u001b[0m][\u001b[32mINFO\u001b[0m] - Processing documents in the column 'tokens'...\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:07:53,478\u001b[0m][\u001b[34mthematos.datasets.corpus\u001b[0m][\u001b[32mINFO\u001b[0m] - 0    [national, bank, cambodia, nbc, ask, bank, fin...\n",
      "1    [national, bank, cambodia, prepare, draft, pro...\n",
      "2    [national, bank, cambodia, prepare, draft, pro...\n",
      "3    [national, bank, cambodia, nbc, monday, sign, ...\n",
      "4    [average, price, good, service, supply, market...\n",
      "Name: tokens, dtype: object\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:07:53,481\u001b[0m][\u001b[34mthematos.datasets.corpus\u001b[0m][\u001b[32mINFO\u001b[0m] - The first document: ['national' 'bank' 'cambodia' 'nbc' 'ask' 'bank' 'financial' 'institution'\n",
      " 'consider' 'provide' 'loan' 'consumer' 'base' 'behaviour' 'data'\n",
      " 'increase' 'financial' 'inclusion' 'digital' 'payment' 'develop' 'fast'\n",
      " 'country' 'recommendation' 'make' 'launch' 'cambodian' 'share' 'switch'\n",
      " 'event' 'common' 'payment' 'withdrawal' 'recently' 'organise' 'center'\n",
      " 'bank' 'study' 'cbs' 'phnom' 'penh' 'preside' 'deputy' 'governor' 'nbc'\n",
      " 'chea' 'serey' 'event' 'also' 'attend' 'national' 'international' 'guest'\n",
      " 'central' 'bank' 'private' 'bank' 'financial' 'institution' 'include'\n",
      " 'sok' 'voeun' 'chairman' 'cambodia' 'microfinance' 'association' 'cma'\n",
      " 'char' 'sopheap' 'payment' 'committee' 'chairman' 'association' 'bank'\n",
      " 'cambodia' 'abc' 'ivana' 'tranchini' 'visa' 'country' 'manager'\n",
      " 'cambodia' 'payment' 'system' 'important' 'financial' 'inclusion'\n",
      " 'process' \"n't\" 'want' 'make' 'payment' 'easy' 'also' 'want' 'make'\n",
      " 'access' 'credit' 'easy' 'comfortable' 'potential' 'look' 'possibility'\n",
      " 'lending' 'provide' 'credit' 'base' 'payment' 'behaviour' 'client' 'talk'\n",
      " 'big' 'amount' 'small' 'amount' 'serey' 'underline' 'could' 'understand'\n",
      " 'behaviour' 'client' 'someone' 'cautious' 'spender' 'someone' 'habit'\n",
      " 'save' 'lot' 'get' 'benefit' 'term' 'low' 'interest' 'rate' 'financial'\n",
      " 'institution' 'hope' 'continue' 'push' 'forward' 'usage' 'data' 'help'\n",
      " 'access' 'financial' 'service' 'serey' 'say' 'serey' 'tell' 'khmer'\n",
      " 'time' 'sideline' 'event' 'bank' 'financial' 'institution' 'currently'\n",
      " 'use' 'necessary' 'data' 'collateral' 'identity' 'income' 'client'\n",
      " 'assess' 'possibility' 'lend' 'data' 'consumer' 'behaviour' 'already'\n",
      " 'exist' 'use' 'assessment' 'decision-making' 'loan' 'release' 'serey'\n",
      " 'far' 'point' 'require' 'bank' 'financial' 'institution' 'must' 'get'\n",
      " 'consent' 'client' 'use' 'type' 'behaviour' 'data' 'protect' 'client'\n",
      " 'privacy' 'client' 'want' 'get' 'loan' 'allow' 'bank' 'use' 'information'\n",
      " 'credit' 'assessment' 'would' 'fine' 'trade-off' 'say' 'example' 'people'\n",
      " 'spend' 'income' 'financial' 'institution' 'use' 'data' 'consider'\n",
      " 'decide' 'lend' 'people' 'behaviour' 'might' 'believe' 'unable' 'make'\n",
      " 'repayment' 'financial' 'institution' 'lender' 'however' 'people' 'habit'\n",
      " 'spending' 'much' 'less' 'income' 'may' 'get' 'credit' 'easy' 'serey'\n",
      " 'add' 'bank' 'financial' 'institution' 'officer' 'also' 'need' 'explain'\n",
      " 'clearly' 'advantage' 'disadvantage' 'customer' 'addition' 'request'\n",
      " 'consent' 'data' 'usage' 'financial' 'service' 'consumer' 'may' 'lack'\n",
      " 'knowledge' 'understand' 'consequence' 'say' 'add' 'officer' 'ask' 'make'\n",
      " 'decision' 'permission' 'serey' 'also' 'say' 'number' 'payment' 'make'\n",
      " 'electronic' 'system' 'cambodia' 'reach' 'nearly' 'percent' 'country'\n",
      " 'gross' 'domestic' 'product' 'gdp' 'rise' 'approximately'\n",
      " 'billion-nearly' 'percent' 'gdp' 'last' 'year' 'electronic' 'payment'\n",
      " 'volume' 'rise' 'percent' 'annually' 'transaction' 'amount' 'payment'\n",
      " 'voeun' 'also' 'ceo' 'deposit-taking' 'microfinance' 'institution' 'lolc'\n",
      " 'cambodia' 'plc' 'ask' 'nbc' 'work' 'participate' 'institution' 'set'\n",
      " 'maximum' 'limit' 'day' 'amount' 'withdrawable' 'cash' 'transaction'\n",
      " 'atm']\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:07:53,740\u001b[0m][\u001b[34mhyfi.utils.iolibs\u001b[0m][\u001b[32mINFO\u001b[0m] - Loaded the file: /home/yjlee/workspace/projects/nbcpu/tests/assets/stopwords/nbcpu-uncertainty.txt, No. of words: 875\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:07:53,741\u001b[0m][\u001b[34mhyfi.utils.iolibs\u001b[0m][\u001b[32mINFO\u001b[0m] - Remove duplicate words, No. of words: 871\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:07:53,741\u001b[0m][\u001b[34mlexikanon.stopwords.stopwords\u001b[0m][\u001b[32mINFO\u001b[0m] - Loaded 871 stopwords from /home/yjlee/workspace/projects/nbcpu/tests/assets/stopwords/nbcpu-uncertainty.txt\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:07:53,741\u001b[0m][\u001b[34mlexikanon.stopwords.stopwords\u001b[0m][\u001b[32mINFO\u001b[0m] - Loaded 871 stopwords\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:03,836\u001b[0m][\u001b[34mthematos.datasets.corpus\u001b[0m][\u001b[32mINFO\u001b[0m] - The first document: ['national' 'bank' 'cambodia' 'nbc' 'ask' 'bank' 'financial' 'institution'\n",
      " 'consider' 'provide' 'loan' 'consumer' 'base' 'behaviour' 'data'\n",
      " 'increase' 'financial' 'inclusion' 'digital' 'payment' 'develop' 'fast'\n",
      " 'country' 'recommendation' 'make' 'launch' 'cambodian' 'share' 'switch'\n",
      " 'event' 'common' 'payment' 'withdrawal' 'recently' 'organise' 'center'\n",
      " 'bank' 'study' 'cbs' 'phnom' 'penh' 'preside' 'deputy' 'governor' 'nbc'\n",
      " 'chea' 'serey' 'event' 'also' 'attend' 'national' 'international' 'guest'\n",
      " 'central' 'bank' 'private' 'bank' 'financial' 'institution' 'include'\n",
      " 'sok' 'voeun' 'chairman' 'cambodia' 'microfinance' 'association' 'cma'\n",
      " 'char' 'sopheap' 'payment' 'committee' 'chairman' 'association' 'bank'\n",
      " 'cambodia' 'abc' 'ivana' 'tranchini' 'visa' 'country' 'manager'\n",
      " 'cambodia' 'payment' 'system' 'important' 'financial' 'inclusion'\n",
      " 'process' \"n't\" 'want' 'make' 'payment' 'easy' 'also' 'want' 'make'\n",
      " 'access' 'credit' 'easy' 'comfortable' 'potential' 'look' 'possibility'\n",
      " 'lending' 'provide' 'credit' 'base' 'payment' 'behaviour' 'client' 'talk'\n",
      " 'big' 'amount' 'small' 'amount' 'serey' 'underline' 'could' 'understand'\n",
      " 'behaviour' 'client' 'someone' 'cautious' 'spender' 'someone' 'habit'\n",
      " 'save' 'lot' 'get' 'benefit' 'term' 'low' 'interest' 'rate' 'financial'\n",
      " 'institution' 'hope' 'continue' 'push' 'forward' 'usage' 'data' 'help'\n",
      " 'access' 'financial' 'service' 'serey' 'say' 'serey' 'tell' 'khmer'\n",
      " 'time' 'sideline' 'event' 'bank' 'financial' 'institution' 'currently'\n",
      " 'use' 'necessary' 'data' 'collateral' 'identity' 'income' 'client'\n",
      " 'assess' 'possibility' 'lend' 'data' 'consumer' 'behaviour' 'already'\n",
      " 'exist' 'use' 'assessment' 'decision-making' 'loan' 'release' 'serey'\n",
      " 'far' 'point' 'require' 'bank' 'financial' 'institution' 'must' 'get'\n",
      " 'consent' 'client' 'use' 'type' 'behaviour' 'data' 'protect' 'client'\n",
      " 'privacy' 'client' 'want' 'get' 'loan' 'allow' 'bank' 'use' 'information'\n",
      " 'credit' 'assessment' 'would' 'fine' 'trade-off' 'say' 'example' 'people'\n",
      " 'spend' 'income' 'financial' 'institution' 'use' 'data' 'consider'\n",
      " 'decide' 'lend' 'people' 'behaviour' 'might' 'believe' 'unable' 'make'\n",
      " 'repayment' 'financial' 'institution' 'lender' 'however' 'people' 'habit'\n",
      " 'spending' 'much' 'less' 'income' 'may' 'get' 'credit' 'easy' 'serey'\n",
      " 'add' 'bank' 'financial' 'institution' 'officer' 'also' 'need' 'explain'\n",
      " 'clearly' 'advantage' 'disadvantage' 'customer' 'addition' 'request'\n",
      " 'consent' 'data' 'usage' 'financial' 'service' 'consumer' 'may' 'lack'\n",
      " 'knowledge' 'understand' 'consequence' 'say' 'add' 'officer' 'ask' 'make'\n",
      " 'decision' 'permission' 'serey' 'also' 'say' 'number' 'payment' 'make'\n",
      " 'electronic' 'system' 'cambodia' 'reach' 'nearly' 'percent' 'country'\n",
      " 'gross' 'domestic' 'product' 'gdp' 'rise' 'approximately'\n",
      " 'billion-nearly' 'percent' 'gdp' 'last' 'year' 'electronic' 'payment'\n",
      " 'volume' 'rise' 'percent' 'annually' 'transaction' 'amount' 'payment'\n",
      " 'voeun' 'also' 'ceo' 'deposit-taking' 'microfinance' 'institution' 'lolc'\n",
      " 'cambodia' 'plc' 'ask' 'nbc' 'work' 'participate' 'institution' 'set'\n",
      " 'maximum' 'limit' 'day' 'amount' 'withdrawable' 'cash' 'transaction'\n",
      " 'atm']\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:04,085\u001b[0m][\u001b[34mthematos.datasets.corpus\u001b[0m][\u001b[32mINFO\u001b[0m] - Total 27594 documents are loaded.\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:04,087\u001b[0m][\u001b[34mthematos.datasets.corpus\u001b[0m][\u001b[32mINFO\u001b[0m] - Extracting n-grams...\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:09,949\u001b[0m][\u001b[34mthematos.datasets.corpus\u001b[0m][\u001b[32mINFO\u001b[0m] - Total 1971 n-grams are extracted.\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:09,951\u001b[0m][\u001b[34mthematos.datasets.corpus\u001b[0m][\u001b[32mINFO\u001b[0m] - tomotopy.label.Candidate(words=[\"prei\",\"kuk\"], name=\"\", score=0.997459)\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:09,951\u001b[0m][\u001b[34mthematos.datasets.corpus\u001b[0m][\u001b[32mINFO\u001b[0m] - tomotopy.label.Candidate(words=[\"ភុន\",\"ច័ន្ទឧសភា\"], name=\"\", score=0.997435)\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:09,951\u001b[0m][\u001b[34mthematos.datasets.corpus\u001b[0m][\u001b[32mINFO\u001b[0m] - tomotopy.label.Candidate(words=[\"cashville\",\"kidz\"], name=\"\", score=0.996442)\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:09,951\u001b[0m][\u001b[34mthematos.datasets.corpus\u001b[0m][\u001b[32mINFO\u001b[0m] - tomotopy.label.Candidate(words=[\"modus\",\"operandi\"], name=\"\", score=0.996309)\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:09,951\u001b[0m][\u001b[34mthematos.datasets.corpus\u001b[0m][\u001b[32mINFO\u001b[0m] - tomotopy.label.Candidate(words=[\"suu\",\"kyi\"], name=\"\", score=0.996141)\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:09,951\u001b[0m][\u001b[34mthematos.datasets.corpus\u001b[0m][\u001b[32mINFO\u001b[0m] - tomotopy.label.Candidate(words=[\"lese\",\"majeste\"], name=\"\", score=0.994870)\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:09,951\u001b[0m][\u001b[34mthematos.datasets.corpus\u001b[0m][\u001b[32mINFO\u001b[0m] - tomotopy.label.Candidate(words=[\"ហ៊ុន\",\"សែន\"], name=\"\", score=0.994573)\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:09,951\u001b[0m][\u001b[34mthematos.datasets.corpus\u001b[0m][\u001b[32mINFO\u001b[0m] - tomotopy.label.Candidate(words=[\"bharatiya\",\"janata\"], name=\"\", score=0.993389)\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:09,951\u001b[0m][\u001b[34mthematos.datasets.corpus\u001b[0m][\u001b[32mINFO\u001b[0m] - tomotopy.label.Candidate(words=[\"los\",\"angeles\"], name=\"\", score=0.989665)\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:09,951\u001b[0m][\u001b[34mthematos.datasets.corpus\u001b[0m][\u001b[32mINFO\u001b[0m] - tomotopy.label.Candidate(words=[\"kuala\",\"lumpur\"], name=\"\", score=0.987660)\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:09,951\u001b[0m][\u001b[34mthematos.datasets.corpus\u001b[0m][\u001b[32mINFO\u001b[0m] - tomotopy.label.Candidate(words=[\"reduced\",\"rapid\"], name=\"\", score=0.501001)\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:09,952\u001b[0m][\u001b[34mthematos.datasets.corpus\u001b[0m][\u001b[32mINFO\u001b[0m] - tomotopy.label.Candidate(words=[\"entry\",\"exit\"], name=\"\", score=0.500969)\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:09,952\u001b[0m][\u001b[34mthematos.datasets.corpus\u001b[0m][\u001b[32mINFO\u001b[0m] - tomotopy.label.Candidate(words=[\"constitutional\",\"amendment\"], name=\"\", score=0.500843)\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:09,952\u001b[0m][\u001b[34mthematos.datasets.corpus\u001b[0m][\u001b[32mINFO\u001b[0m] - tomotopy.label.Candidate(words=[\"governor\",\"keut\"], name=\"\", score=0.500629)\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:09,952\u001b[0m][\u001b[34mthematos.datasets.corpus\u001b[0m][\u001b[32mINFO\u001b[0m] - tomotopy.label.Candidate(words=[\"severely\",\"affect\"], name=\"\", score=0.500479)\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:09,952\u001b[0m][\u001b[34mthematos.datasets.corpus\u001b[0m][\u001b[32mINFO\u001b[0m] - tomotopy.label.Candidate(words=[\"liberation\",\"front\"], name=\"\", score=0.500423)\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:09,952\u001b[0m][\u001b[34mthematos.datasets.corpus\u001b[0m][\u001b[32mINFO\u001b[0m] - tomotopy.label.Candidate(words=[\"emphasize\",\"importance\"], name=\"\", score=0.500367)\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:09,952\u001b[0m][\u001b[34mthematos.datasets.corpus\u001b[0m][\u001b[32mINFO\u001b[0m] - tomotopy.label.Candidate(words=[\"weapon\",\"mass\"], name=\"\", score=0.500308)\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:09,952\u001b[0m][\u001b[34mthematos.datasets.corpus\u001b[0m][\u001b[32mINFO\u001b[0m] - tomotopy.label.Candidate(words=[\"highlight\",\"importance\"], name=\"\", score=0.500183)\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:09,952\u001b[0m][\u001b[34mthematos.datasets.corpus\u001b[0m][\u001b[32mINFO\u001b[0m] - tomotopy.label.Candidate(words=[\"atm\",\"machine\"], name=\"\", score=0.500026)\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:09,952\u001b[0m][\u001b[34mthematos.datasets.corpus\u001b[0m][\u001b[32mINFO\u001b[0m] - Concatenating n-grams...\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:09,952\u001b[0m][\u001b[34mthematos.datasets.corpus\u001b[0m][\u001b[32mINFO\u001b[0m] - <tomotopy.Document with words=\"ask consider provide base behaviour increase inclusion fast recommendation launch switch common withdrawal recently organise center cbs preside deputy governor chea serey attend guest voeun chairman cma char sopheap chairman abc ivana tranchini manager inclusion process easy easy comfortable potential look possibility provide base behaviour talk amount amount serey underline understand behaviour someone cautious spender someone habit save lot benefit term hope push forward usage serey serey sideline currently necessary collateral identity income assess possibility lend behaviour exist assessment decision-making release serey require consent type behaviour protect privacy assessment fine trade-off example spend income consider decide lend behaviour might believe unable repayment lender habit less income easy serey add explain clearly advantage disadvantage addition request consent usage lack knowledge understand consequence add ask permission serey electronic nearly gross domestic approximately billion-nearly electronic annually amount voeun ceo deposit-taking lolc ask participate maximum limit amount withdrawable atm\">\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:09,953\u001b[0m][\u001b[34mthematos.datasets.corpus\u001b[0m][\u001b[32mINFO\u001b[0m] - <tomotopy.Document with words=\"san francisco march twenty-five ago wide web idea technical paper obscure scientist physic lab idea tim berners-lee cern lab switzerland outline easily file linked pave phenomenon touch billion present paper march mark birthday web idea bold almost happen tremendous amount hubris beginning marc weber creator curator silicon valley tim berners-lee propose blue unrequested weber cern colleague completely ignore proposal web rival begin idea connect launched arpanet forerunner wide web several idea connect berners-lee convince cern adopt demonstrate usefulness compile lab phone aspect design put forward berners-lee across various operate ability click link file host locate elsewhere web winner gate rival us-based compuserve minitel involve fee berners-lee free real underdog one predict succeed weber gopher minnesota beat web early gore white weber vice gore web topple gopher agency launch whitehouse gov website huge stamp approval web web release free gopher weber realize web competitor weber lose battle certainly different lot top-down wall garden web competitor berners-lee free publish wish internet-linked titan yahoo page amount host server explode disrupt birth guilty lack imagination web gartner michael mcguire personal web disrupt lot ability freely file web shake traditional music film push edge jim dempsey vice us-based center anybody listener anybody publisher never anything powerful underlying tenet web egalitarian principle dempsey remain web hobble regulation fragment wall portion freedom threaten never stop teenage kid watch little snippet cute dempsey trouble limit ability criticize tiered hard innovator critic activist audience threats web base equality concern creator weber web unify decade ago nothing write stone fragment anew historian reason provider win preferential willingness invade privacy restrain web freedom battle shape web effect billion smartphones web really half worldwide yet weber\">\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:10,248\u001b[0m][\u001b[34mthematos.datasets.corpus\u001b[0m][\u001b[32mINFO\u001b[0m] - <tomotopy.Document with words=\"ask consider provide base behaviour increase inclusion fast recommendation launch switch common withdrawal recently organise center cbs preside deputy_governor chea_serey attend guest voeun chairman cma char sopheap chairman abc ivana tranchini manager inclusion process easy easy comfortable potential look possibility provide base behaviour talk amount amount serey underline understand behaviour someone cautious spender someone habit save lot benefit term hope push forward usage serey serey sideline currently necessary collateral identity income assess possibility lend behaviour exist assessment decision-making release serey require consent type behaviour protect privacy assessment fine trade-off example spend income consider decide lend behaviour might believe unable repayment lender habit less income easy serey add explain clearly advantage disadvantage addition request consent usage lack knowledge understand consequence add ask permission serey electronic nearly gross_domestic approximately billion-nearly electronic annually amount voeun ceo deposit-taking lolc ask participate maximum limit amount withdrawable atm\">\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:10,248\u001b[0m][\u001b[34mthematos.datasets.corpus\u001b[0m][\u001b[32mINFO\u001b[0m] - <tomotopy.Document with words=\"san_francisco march twenty-five ago wide web idea technical paper obscure scientist physic lab idea tim berners-lee cern lab switzerland outline easily file linked pave phenomenon touch billion present paper march mark birthday web idea bold almost happen tremendous amount hubris beginning marc weber creator curator silicon_valley tim berners-lee propose blue unrequested weber cern colleague completely ignore proposal web rival begin idea connect launched arpanet forerunner wide web several idea connect berners-lee convince cern adopt demonstrate usefulness compile lab phone aspect design put forward berners-lee across various operate ability click link file host locate elsewhere web winner gate rival us-based compuserve minitel involve fee berners-lee free real underdog one predict succeed weber gopher minnesota beat web early gore white weber vice gore web topple gopher agency launch whitehouse gov website huge stamp approval web web release free gopher weber realize web competitor weber lose battle certainly different lot top-down wall garden web competitor berners-lee free publish wish internet-linked titan yahoo page amount host server explode disrupt birth guilty lack imagination web gartner michael mcguire personal web disrupt lot ability freely file web shake traditional music film push edge jim dempsey vice us-based center anybody listener anybody publisher never anything powerful underlying tenet web egalitarian principle dempsey remain web hobble regulation fragment wall portion freedom threaten never stop teenage kid watch little snippet cute dempsey trouble limit ability criticize tiered hard innovator critic activist audience threats web base equality concern creator weber web unify decade ago nothing write stone fragment anew historian reason provider win preferential willingness invade privacy restrain web freedom battle shape web effect billion smartphones web really half worldwide yet weber\">\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:10,248\u001b[0m][\u001b[34mthematos.datasets.corpus\u001b[0m][\u001b[32mINFO\u001b[0m] - Total 27594 documents are n-gramized.\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:10,250\u001b[0m][\u001b[34mhyfi.utils.datasets.save\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving dataframe to /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_uncertainty/corpus/corpus_doc_ids.parquet\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:10,273\u001b[0m][\u001b[34mhyfi.utils.datasets.save\u001b[0m][\u001b[32mINFO\u001b[0m] -  >> elapsed time to save data: 0:00:00.022347\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:10,274\u001b[0m][\u001b[34mhyfi.composer.config\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving config to /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_uncertainty/corpus/configs/corpus(1)_config.json\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:10,274\u001b[0m][\u001b[34mhyfi.composer.config\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving config to /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_uncertainty/corpus/configs/corpus(1)_config.yaml\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:10,871\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - Set words ['enhance', 'ensure', 'improve', 'strengthen'] to topic #1 as prior.\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:11,425\u001b[0m][\u001b[34mthematos.models.lda\u001b[0m][\u001b[32mINFO\u001b[0m] - Number of docs: 27594\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:11,425\u001b[0m][\u001b[34mthematos.models.lda\u001b[0m][\u001b[32mINFO\u001b[0m] - Vocab size: 1102\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:11,425\u001b[0m][\u001b[34mthematos.models.lda\u001b[0m][\u001b[32mINFO\u001b[0m] - Number of words: 1916988\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:11,425\u001b[0m][\u001b[34mthematos.models.lda\u001b[0m][\u001b[32mINFO\u001b[0m] - Removed top words: []\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:11,425\u001b[0m][\u001b[34mthematos.models.lda\u001b[0m][\u001b[32mINFO\u001b[0m] - Training model by iterating over the corpus 100 times, 10 iterations at a time with 0 workers\u001b[0m\n",
      "\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A[\u001b[36m2023-08-15 19:09:15,120\u001b[0m][\u001b[34mthematos.models.lda\u001b[0m][\u001b[32mINFO\u001b[0m] - Iteration: 0\tLog-likelihood: -7.758692721106333\u001b[0m\n",
      "\n",
      " 10%|████▍                                       | 1/10 [00:03<00:33,  3.70s/it]\u001b[A[\u001b[36m2023-08-15 19:09:18,869\u001b[0m][\u001b[34mthematos.models.lda\u001b[0m][\u001b[32mINFO\u001b[0m] - Iteration: 10\tLog-likelihood: -7.343293556088631\u001b[0m\n",
      "\n",
      " 20%|████████▊                                   | 2/10 [00:07<00:29,  3.73s/it]\u001b[A[\u001b[36m2023-08-15 19:09:22,708\u001b[0m][\u001b[34mthematos.models.lda\u001b[0m][\u001b[32mINFO\u001b[0m] - Iteration: 20\tLog-likelihood: -7.2300475972122795\u001b[0m\n",
      "\n",
      " 30%|█████████████▏                              | 3/10 [00:11<00:26,  3.78s/it]\u001b[A[\u001b[36m2023-08-15 19:09:26,708\u001b[0m][\u001b[34mthematos.models.lda\u001b[0m][\u001b[32mINFO\u001b[0m] - Iteration: 30\tLog-likelihood: -7.181403094694064\u001b[0m\n",
      "\n",
      " 40%|█████████████████▌                          | 4/10 [00:15<00:23,  3.87s/it]\u001b[A[\u001b[36m2023-08-15 19:09:30,682\u001b[0m][\u001b[34mthematos.models.lda\u001b[0m][\u001b[32mINFO\u001b[0m] - Iteration: 40\tLog-likelihood: -7.15484932973934\u001b[0m\n",
      "\n",
      " 50%|██████████████████████                      | 5/10 [00:19<00:19,  3.91s/it]\u001b[A[\u001b[36m2023-08-15 19:09:34,609\u001b[0m][\u001b[34mthematos.models.lda\u001b[0m][\u001b[32mINFO\u001b[0m] - Iteration: 50\tLog-likelihood: -7.137309273065852\u001b[0m\n",
      "\n",
      " 60%|██████████████████████████▍                 | 6/10 [00:23<00:15,  3.91s/it]\u001b[A[\u001b[36m2023-08-15 19:09:38,468\u001b[0m][\u001b[34mthematos.models.lda\u001b[0m][\u001b[32mINFO\u001b[0m] - Iteration: 60\tLog-likelihood: -7.122647192782161\u001b[0m\n",
      "\n",
      " 70%|██████████████████████████████▊             | 7/10 [00:27<00:11,  3.90s/it]\u001b[A[\u001b[36m2023-08-15 19:09:42,328\u001b[0m][\u001b[34mthematos.models.lda\u001b[0m][\u001b[32mINFO\u001b[0m] - Iteration: 70\tLog-likelihood: -7.112307842481844\u001b[0m\n",
      "\n",
      " 80%|███████████████████████████████████▏        | 8/10 [00:30<00:07,  3.88s/it]\u001b[A[\u001b[36m2023-08-15 19:09:46,239\u001b[0m][\u001b[34mthematos.models.lda\u001b[0m][\u001b[32mINFO\u001b[0m] - Iteration: 80\tLog-likelihood: -7.103769303813615\u001b[0m\n",
      "\n",
      " 90%|███████████████████████████████████████▌    | 9/10 [00:34<00:03,  3.89s/it]\u001b[A[\u001b[36m2023-08-15 19:09:50,140\u001b[0m][\u001b[34mthematos.models.lda\u001b[0m][\u001b[32mINFO\u001b[0m] - Iteration: 90\tLog-likelihood: -7.095995534976736\u001b[0m\n",
      "\n",
      "100%|███████████████████████████████████████████| 10/10 [00:38<00:00,  3.87s/it]\u001b[A\n",
      "<Basic Info>\n",
      "| LDAModel (current version: 0.12.5)\n",
      "| 27594 docs, 1916988 words\n",
      "| Total Vocabs: 131732, Used Vocabs: 1102\n",
      "| Entropy of words: 6.75978\n",
      "| Entropy of term-weighted words: 6.90695\n",
      "| Removed Vocabs: <NA>\n",
      "|\n",
      "<Training Info>\n",
      "| Iterations: 100, Burn-in steps: 0\n",
      "| Optimization Interval: 10\n",
      "| Log-likelihood per word: -7.09600\n",
      "|\n",
      "<Initial Parameters>\n",
      "| tw: TermWeight.IDF\n",
      "| min_cf: 500 (minimum collection frequency of words)\n",
      "| min_df: 500 (minimum document frequency of words)\n",
      "| rm_top: 0 (the number of top words to be removed)\n",
      "| k: 20 (the number of topics between 1 ~ 32767)\n",
      "| alpha: [0.1] (hyperparameter of Dirichlet distribution for document-topic, given as a single `float` in case of symmetric prior and as a list with length `k` of `float` in case of asymmetric prior.)\n",
      "| eta: 0.01 (hyperparameter of Dirichlet distribution for topic-word)\n",
      "| seed: 590167864 (random seed)\n",
      "| trained in version 0.12.5\n",
      "|\n",
      "<Parameters>\n",
      "| alpha (Dirichlet prior on the per-document topic distributions)\n",
      "|  [0.02006483 0.04694471 0.01432557 0.00799661 0.01058124 0.01064451\n",
      "|   0.00866108 0.00949337 0.00843484 0.00827856 0.0124673  0.01257223\n",
      "|   0.01054008 0.01247833 0.01060978 0.01064283 0.01324215 0.01039483\n",
      "|   0.01151689 0.01167775]\n",
      "| eta (Dirichlet prior on the per-topic word distribution)\n",
      "|  0.01\n",
      "|\n",
      "<Topics>\n",
      "| #0 (174430) : risk slow recovery cut remain fall fiscal weak hike likely slowdown grow strong domestic recession decline uncertainty increase survey fell\n",
      "| #1 (370043) : improve provide strengthen ensure enhance achieve implement increase contribute aim framework reduce goal address technical benefit forum strategic stakeholder financing\n",
      "| #2 (91486) : increase zone add special siem_reap provide attract korean invest produce expand boost ltd director complete facility collect income minimum operation\n",
      "| #3 (48287) : dispute arrest gold claim resolve ask accuse trust appeal title sell file send name seize involve fine mother problem try\n",
      "| #4 (95614) : attack reform citizen fact respect fight problem think win never organisation protect bear view act always clear sam word understand\n",
      "| #5 (76207) : list korean increase domestic negotiation free minimum boost incentive renewable raise stake attract potential reduce invest sign asset value footwear\n",
      "| #6 (59974) : accident hall ask concern protest spokesman add representative commission request ban process block claim accept hold remove administration assembly affect\n",
      "| #7 (68521) : kill arrest live leave attack die night try ask back never stop sell later mother lose away incident think protest\n",
      "| #8 (63445) : protest activist win freedom reform movement rally attack ban cabinet sanction poll claim assembly accuse sunday civil act hold post\n",
      "| #9 (53266) : fell york gain trader cut wall rally equity earnings drop loss fall profit hike yield hit corp surge record weak\n",
      "| #10 (102431) : live problem affect poor cause often think risk damage less bad enough long happen back die protect loss severe leave\n",
      "| #11 (88649) : siem_reap park kampot complete sell facility centre route connect phase locate produce ship resort hour add center line koh link\n",
      "| #12 (70812) : daily record reopen cent announce vaccinate today recovery drop ban remain household situation germany normal increase surge cause poor direct\n",
      "| #13 (62981) : increase abc respectively drop gain value compare pwsa gti period decline year-on-year decrease fell compare_period record fall footwear profit finish\n",
      "| #14 (63674) : arrest kandal kill live accuse send night son die ask koh morning body accident incident sell leave collapse vaccinate identify\n",
      "| #15 (68178) : article draft document accuse arrest request assembly fine appeal code submit ask procedure file register permit convict add send require\n",
      "| #16 (114897) : youth experience provide best launch benefit feature team grow ceo different learn together idea design foundation example various traditional understand\n",
      "| #17 (70406) : asset transfer ceo shareholder corporate regulator list trust executive provide portfolio launch operation grow best provider experience staff value agent\n",
      "| #18 (92694) : real_estate buy agent purchase transfer grow store provide ceo look affordable launch easy option experience re sell available invest convenient\n",
      "| #19 (80993) : request assembly register add draft affair spokesman civil sam reform process deputy discuss administration relevant submit ask hold procedure governor\n",
      "|\n",
      "\n",
      "[\u001b[36m2023-08-15 19:09:50,965\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - ==== Coherence : u_mass ====\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:50,965\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - Average: -2.0675997963694988\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:50,965\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - Per Topic: [-1.9086247983459215, -1.3976213790853773, -2.1882474353474812, -2.5721022891995746, -1.9975944984014402, -2.7226100753271103, -2.0055773674467345, -1.6916198990274103, -2.4384248735913547, -1.8001375528751784, -1.826303930648957, -2.4255806540436002, -2.220352322267821, -2.1150062333273456, -2.0433877722588516, -2.2476214746839625, -1.8547688291891224, -1.8227270593580585, -1.799965619790992, -2.2737218631736935]\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:51,439\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - ==== Coherence : c_uci ====\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:51,440\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - Average: 0.5407962501771852\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:51,440\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - Per Topic: [0.7980955958033914, 0.6568717860517432, 0.37871382559844646, 0.24468810769433685, 0.24671037142181412, -0.5484771488713698, 0.2858446921351938, 0.5664955644072326, 0.4830281137375432, 1.3018222798657366, 0.39453938957949164, 0.6325481265318617, 0.37858146150510924, 1.7849392823267523, 0.7705559474225172, 0.9236258523830794, 0.13867486010456936, 0.6123869983666509, 0.2811976643458761, 0.4850822331337288]\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:51,893\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - ==== Coherence : c_npmi ====\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:51,894\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - Average: 0.066997714678115\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:51,894\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - Per Topic: [0.08972189379830393, 0.08460459654168709, 0.04861142683257556, 0.03847374976484533, 0.027179217297177302, 0.00026794000017148797, 0.03754634335139229, 0.06217883791201524, 0.05328677116199519, 0.13792800009146242, 0.04825418436434792, 0.06778716593319205, 0.05224243061223199, 0.22305745521267398, 0.08509781648733664, 0.11051469073545851, 0.018417100260943384, 0.06588315923933655, 0.035715388916862274, 0.05318612504829075]\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:54,502\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - ==== Coherence : c_v ====\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:54,503\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - Average: 0.6176439623348415\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:54,503\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - Per Topic: [0.7311489164829255, 0.7208846032619476, 0.5343140661716461, 0.5964014139026403, 0.5790759354829789, 0.48090492784976957, 0.5460225224494935, 0.7206535756587982, 0.6764586597681046, 0.7651297628879548, 0.5198783069849015, 0.5806703731417656, 0.4698037400841713, 0.5925369501113892, 0.7583050549030304, 0.6973470628261567, 0.5783749014139176, 0.6185191124677658, 0.5549376428127288, 0.6315117180347443]\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:54,554\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - Model saved to /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_uncertainty/model/models/LDA_model(4)_k(20).mdl\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:54,605\u001b[0m][\u001b[34mhyfi.utils.datasets.save\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving dataframe to /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_uncertainty/model/outputs/LDA_model(4)_k(20)-ll_per_word.csv\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:54,607\u001b[0m][\u001b[34mhyfi.utils.datasets.save\u001b[0m][\u001b[32mINFO\u001b[0m] -  >> elapsed time to save data: 0:00:00.001788\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:55,011\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - Log-likelihood per word plot saved to /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_uncertainty/model/outputs/LDA_model(4)_k(20)-ll_per_word.png\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:55,079\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - ==== Document-Topic Distributions ====\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:55,079\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] -           id    topic0    topic1  ...   topic17   topic18   topic19\n",
      "27589  48239  0.000176  0.000412  ...  0.000091  0.000101  0.000102\n",
      "27590  48216  0.000160  0.000375  ...  0.000083  0.000092  0.000093\n",
      "27591  48132  0.000061  0.019791  ...  0.000032  0.000035  0.178651\n",
      "27592  48115  0.258588  0.740238  ...  0.000063  0.000069  0.000070\n",
      "27593  48085  0.000056  0.000132  ...  0.000029  0.000032  0.046560\n",
      "\n",
      "[5 rows x 21 columns]\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:55,108\u001b[0m][\u001b[34mhyfi.utils.datasets.save\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving dataframe to /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_uncertainty/model/outputs/LDA_model(4)_k(20)-doc_topic_dists.parquet\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:55,272\u001b[0m][\u001b[34mhyfi.utils.datasets.save\u001b[0m][\u001b[32mINFO\u001b[0m] -  >> elapsed time to save data: 0:00:00.164053\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:55,272\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - ==== Topic-Word Distributions ====\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:55,273\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] -          add  increase   provide  ...        actual     elsewhere     seriously\n",
      "15  0.005102  0.000574  0.002074  ...  6.625699e-04  5.167474e-08  9.678460e-04\n",
      "16  0.002256  0.002083  0.004857  ...  3.056953e-08  2.804690e-04  3.048550e-04\n",
      "17  0.002904  0.003924  0.006911  ...  4.072869e-04  1.013075e-04  5.078265e-08\n",
      "18  0.003353  0.003373  0.005500  ...  1.029137e-03  6.048432e-04  3.891736e-08\n",
      "19  0.005899  0.001628  0.003265  ...  8.575640e-04  4.465753e-08  7.463032e-04\n",
      "\n",
      "[5 rows x 1102 columns]\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:55,373\u001b[0m][\u001b[34mhyfi.utils.datasets.save\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving dataframe to /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_uncertainty/model/outputs/LDA_model(4)_k(20)-topic_term_dists.parquet\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:55,499\u001b[0m][\u001b[34mhyfi.utils.datasets.save\u001b[0m][\u001b[32mINFO\u001b[0m] -  >> elapsed time to save data: 0:00:00.125595\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:55,502\u001b[0m][\u001b[34mhyfi.utils.iolibs\u001b[0m][\u001b[32mINFO\u001b[0m] - Save the list to the file: /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_uncertainty/model/outputs/LDA_model(4)_k(20)-used_vocab.txt, no. of words: 1102\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:55,505\u001b[0m][\u001b[34mhyfi.utils.iolibs\u001b[0m][\u001b[32mINFO\u001b[0m] - Save the list to the file: /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_uncertainty/model/outputs/LDA_model(4)_k(20)-topic_top_words.txt, no. of words: 497\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:55,506\u001b[0m][\u001b[34mhyfi.utils.datasets.save\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving dataframe to /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_uncertainty/model/outputs/LDA_model(4)_k(20)-topic_top_words_dists.csv\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:09:55,509\u001b[0m][\u001b[34mhyfi.utils.datasets.save\u001b[0m][\u001b[32mINFO\u001b[0m] -  >> elapsed time to save data: 0:00:00.002590\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:10:10,546\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - Making wordcloud collage with titles: ['Topic 0', 'Topic 1', 'Topic 2', 'Topic 3', 'Topic 4', 'Topic 5', 'Topic 6', 'Topic 7', 'Topic 8', 'Topic 9', 'Topic 10', 'Topic 11', 'Topic 12', 'Topic 13', 'Topic 14', 'Topic 15', 'Topic 16', 'Topic 17', 'Topic 18', 'Topic 19']\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:10:10,546\u001b[0m][\u001b[34mhyfi.graphics.collage\u001b[0m][\u001b[32mINFO\u001b[0m] - Making page 1/1 with 20 images\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:10:10,546\u001b[0m][\u001b[34mhyfi.graphics.collage\u001b[0m][\u001b[32mINFO\u001b[0m] - Page titles: ['Topic 0', 'Topic 1', 'Topic 2', 'Topic 3', 'Topic 4', 'Topic 5', 'Topic 6', 'Topic 7', 'Topic 8', 'Topic 9', 'Topic 10', 'Topic 11', 'Topic 12', 'Topic 13', 'Topic 14', 'Topic 15', 'Topic 16', 'Topic 17', 'Topic 18', 'Topic 19']\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:10:10,546\u001b[0m][\u001b[34mhyfi.graphics.collage\u001b[0m][\u001b[32mINFO\u001b[0m] - Page output file: /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_uncertainty/model/outputs/wordcloud_collage/LDA_model(4)_k(20)_wordcloud_00.png\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:10:17,977\u001b[0m][\u001b[34mhyfi.graphics.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - Saved subplots to /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_uncertainty/model/outputs/wordcloud_collage/LDA_model(4)_k(20)_wordcloud_00.png\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:10:17,978\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[33mWARNING\u001b[0m] - pyLDAvis is not installed. Please install it to save LDAvis.\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:10:17,994\u001b[0m][\u001b[34mthematos.models.base\u001b[0m][\u001b[32mINFO\u001b[0m] - Model summary saved to /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_uncertainty/model/outputs/model-summary.jsonl\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:10:17,994\u001b[0m][\u001b[34mhyfi.composer.config\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving config to /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_uncertainty/model/configs/model(4)_config.json\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:10:17,995\u001b[0m][\u001b[34mhyfi.composer.config\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving config to /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_uncertainty/model/configs/model(4)_config.yaml\u001b[0m\n",
      "100%|████████████████████████████████████████████| 1/1 [02:26<00:00, 146.74s/it]\n",
      "[\u001b[36m2023-08-15 19:10:18,019\u001b[0m][\u001b[34mthematos.runners.topic\u001b[0m][\u001b[32mINFO\u001b[0m] - Saved summaries to /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_uncertainty/runner(1)_summaries.json\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:10:18,019\u001b[0m][\u001b[34mhyfi.composer.config\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving config to /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_uncertainty/runner/configs/runner(1)_config.json\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:10:18,019\u001b[0m][\u001b[34mhyfi.composer.config\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving config to /home/yjlee/workspace/projects/nbcpu/workspace/nbcpu-topic_uncertainty/runner/configs/runner(1)_config.yaml\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!nbcpu +workflow=nbcpu tasks='[nbcpu-topic_uncertainty]' mode=__info__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Results\n",
    "\n",
    "The specialized Latent Dirichlet Allocation (LDA) model, designed to classify uncertainty, was applied to a corpus of 27,594 documents containing 1,916,988 words. Out of 131,732 total vocabs, 1,102 were used in the analysis, with specific parameters set to focus on uncertainty-related topics.\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Topic #0**: This topic prominently features terms directly related to uncertainty, such as \"risk,\" \"slow recovery,\" \"recession,\" \"uncertainty,\" and economic fluctuations like \"cut,\" \"hike,\" \"fall,\" \"increase,\" and \"decline.\" It encapsulates the economic uncertainty and potential risks in recovery and growth.\n",
    "\n",
    "2. **Topic #1**: This topic aligns with the prior emphasizing improvement and strengthening. Terms like \"improve,\" \"strengthen,\" \"ensure,\" \"achieve,\" and \"enhance\" reflect efforts to mitigate uncertainty by enhancing frameworks, addressing goals, and implementing reforms.\n",
    "\n",
    "3. **Topic Coherence Scores**: The coherence scores, including u_mass at -2.0798, c_uci at 0.4653, c_npmi at 0.0657, and c_v at 0.5938, indicate a reasonable level of interpretability and relevance of the topics, although there may be room for further refinement.\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "The results demonstrate the model's effectiveness in identifying and classifying uncertainty-related topics. The incorporation of prior knowledge, removal of specific stop words, and fine-tuning of model parameters have led to the extraction of themes that resonate with the concept of uncertainty.\n",
    "\n",
    "Topic #0 provides a comprehensive view of economic uncertainty, capturing the volatility and risks in the market. Topic #1, on the other hand, offers insights into strategies and efforts to navigate and mitigate uncertainty.\n",
    "\n",
    "The tailored approach to topic modeling for uncertainty has yielded meaningful insights into the themes of risk, recovery, and strategies to overcome uncertainty. The model's configuration and the resulting topics align well with the research objectives, providing a nuanced understanding of uncertainty within the given context. The coherence scores suggest that the topics are interpretable, although further refinement and exploration may enhance the model's precision and depth of analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{numref}`fig-lda-wordcloud-uncertainty` shows the wordcloud of the top 500 words in each topic from the LDA model with 20 topics and uncertainty prior.\n",
    "\n",
    "```{figure} ./figs/LDA_model(3)_k(20)_wordcloud_00.png\n",
    "---\n",
    "name: fig-lda-wordcloud-uncertainty\n",
    "---\n",
    "Wordcloud of the top 500 words in each topic from the LDA model with 20 topics and uncertainty prior.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refinement of Uncertainty Topic Modeling\n",
    "\n",
    "The refinement of uncertainty topic modeling is executed through a methodical two-stage process. Initially, documents are filtered according to specific criteria, and subsequently, the topic model is reapplied to this refined dataset. This iterative methodology sharpens the focus on themes pertinent to uncertainty, yielding a more accurate and detailed analysis.\n",
    "\n",
    "In the first stage, documents are identified and filtered based on their relevance to uncertainty topics, specifically topics 0 and 1. The selection is guided by the combined weight of these topics, which directly correspond to the concept of uncertainty. Following this identification, the selected documents are merged with the original dataset.\n",
    "\n",
    "The final stage of the process involves a further filtering of the data, adhering to the established selection criteria, and preparing the dataset for another round of topic modeling. This is achieved by applying the query `\"topic_relevant > 0.5\"`, ensuring that only documents meeting this threshold are retained.\n",
    "\n",
    "This systematic refinement process epitomizes a targeted approach to uncertainty topic modeling. It emphasizes the selection of documents that resonate with the themes of risk, uncertainty, and strategic mitigation. By employing this iterative and filtering technique, the analysis is honed in on the essential facets of uncertainty, thereby improving both the precision and interpretability of the findings. The configuration of the pipeline fortifies this approach, guaranteeing a methodical and replicable procedure that is in harmony with the overarching research goals and academic standards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[36m2023-08-15 19:16:38,472\u001b[0m][\u001b[34mhyfi.joblib.joblib\u001b[0m][\u001b[32mINFO\u001b[0m] - initialized batcher with <hyfi.joblib.batch.batcher.Batcher object at 0x7f27d820e610>\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:16:38,473\u001b[0m][\u001b[34mhyfi.main.config\u001b[0m][\u001b[32mINFO\u001b[0m] - HyFi project [nbcpu] initialized\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:16:38,669\u001b[0m][\u001b[34mhyfi.main.main\u001b[0m][\u001b[32mINFO\u001b[0m] - The HyFI config is not instantiatable, running HyFI task with the config\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:16:39,529\u001b[0m][\u001b[34mhyfi.joblib.joblib\u001b[0m][\u001b[32mINFO\u001b[0m] - initialized batcher with <hyfi.joblib.batch.batcher.Batcher object at 0x7f27b8350760>\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:16:40,726\u001b[0m][\u001b[34mhyfi.workflow.workflow\u001b[0m][\u001b[32mINFO\u001b[0m] - Running task [nbcpu-datasets_uncertainty_filter] with [run={} verbose=False uses='nbcpu-datasets_uncertainty_filter']\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:16:40,750\u001b[0m][\u001b[34mhyfi.task.task\u001b[0m][\u001b[32mINFO\u001b[0m] - Running 1 pipeline(s)\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:16:40,750\u001b[0m][\u001b[34mhyfi.task.task\u001b[0m][\u001b[32mINFO\u001b[0m] - Running pipeline: nbcpu-datasets_uncertainty_filter\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:16:40,762\u001b[0m][\u001b[34mhyfi.task.task\u001b[0m][\u001b[32mINFO\u001b[0m] - Applying 6 pipes: [{'_target_': 'hyfi.utils.datasets.load.DSLoad.load_dataframes', 'data_files': 'nbcpu-topic_uncertainty/model/outputs/LDA_model(3)_k(20)-doc_topic_dists.parquet', 'data_dir': None, 'filetype': None, 'split': None, 'concatenate': False, 'ignore_index': False, 'use_cached': False, 'verbose': True}, {'_target_': 'hyfi.utils.datasets.basic.DSBasic.dataframe_eval_columns', 'expressions': {'topic_relevant': 'topic0 + topic1'}, 'engine': 'python', 'verbose': True}, {'_target_': 'hyfi.utils.datasets.basic.DSBasic.dataframe_select_columns', 'columns': ['id', 'topic_relevant'], 'verbose': True}, {'_target_': 'hyfi.utils.datasets.combine.DSCombine.merge_dataframes', 'right': 'datasets/processed/khmer_tokenized/train.parquet', 'how': 'inner', 'on': 'id', 'left_on': None, 'right_on': None, 'left_index': False, 'right_index': False, 'sort': False, 'suffixes': ['_x', '_y'], 'copy': True, 'indicator': False, 'validate': None, 'verbose': True}, {'_target_': 'hyfi.utils.datasets.basic.DSBasic.dataframe_print_head_and_tail', 'num_heads': 5, 'num_tails': 5, 'columns': ['id', 'text', 'tokens', 'topic_relevant'], 'verbose': True}, {'_target_': 'hyfi.utils.datasets.slice.DSSlice.filter_and_sample_data', 'queries': ['topic_relevant > 0.5'], 'sample_size': None, 'sample_seed': 42, 'output_dir': 'datasets/processed/topic_uncertainty_filtered', 'sample_filename': None, 'train_filename': 'train.parquet', 'discard_filename': 'discard.parquet', 'returning_data': 'train', 'verbose': True}]\u001b[0m\n",
      " Change directory to /home/yjlee/workspace/projects/nbcpu/workspace\n",
      "\u001b[0m[\u001b[36m2023-08-15 19:16:40,767\u001b[0m][\u001b[34mhyfi.pipeline.config\u001b[0m][\u001b[32mINFO\u001b[0m] - Returning partial function: hyfi.utils.datasets.load.DSLoad.load_dataframes with kwargs: {'_target_': 'hyfi.utils.datasets.load.DSLoad.load_dataframes', 'data_files': 'nbcpu-topic_uncertainty/model/outputs/LDA_model(3)_k(20)-doc_topic_dists.parquet', 'data_dir': None, 'filetype': None, 'split': None, 'concatenate': False, 'ignore_index': False, 'use_cached': False, 'verbose': True}\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:16:40,767\u001b[0m][\u001b[34mhyfi.composer.composer\u001b[0m][\u001b[32mINFO\u001b[0m] - instantiating hyfi.utils.datasets.load.DSLoad.load_dataframes ...\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:16:40,770\u001b[0m][\u001b[34mhyfi.utils.iolibs\u001b[0m][\u001b[32mINFO\u001b[0m] - Processing [1] files from ['nbcpu-topic_uncertainty/model/outputs/LDA_model(3)_k(20)-doc_topic_dists.parquet']\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:16:40,770\u001b[0m][\u001b[34mhyfi.utils.datasets.load\u001b[0m][\u001b[32mINFO\u001b[0m] - Loading data from nbcpu-topic_uncertainty/model/outputs/LDA_model(3)_k(20)-doc_topic_dists.parquet\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:16:40,791\u001b[0m][\u001b[34mhyfi.utils.datasets.load\u001b[0m][\u001b[32mINFO\u001b[0m] -  >> elapsed time to load data: 0:00:00.020824\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:16:40,793\u001b[0m][\u001b[34mhyfi.pipeline.config\u001b[0m][\u001b[32mINFO\u001b[0m] - Returning partial function: hyfi.utils.datasets.basic.DSBasic.dataframe_eval_columns with kwargs: {'_target_': 'hyfi.utils.datasets.basic.DSBasic.dataframe_eval_columns', 'expressions': {'topic_relevant': 'topic0 + topic1'}, 'engine': 'python', 'verbose': True}\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:16:40,793\u001b[0m][\u001b[34mhyfi.composer.composer\u001b[0m][\u001b[32mINFO\u001b[0m] - instantiating hyfi.utils.datasets.basic.DSBasic.dataframe_eval_columns ...\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:16:40,795\u001b[0m][\u001b[34mhyfi.utils.datasets.basic\u001b[0m][\u001b[32mINFO\u001b[0m] - Evaluating column topic_relevant\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:16:40,801\u001b[0m][\u001b[34mhyfi.pipeline.config\u001b[0m][\u001b[32mINFO\u001b[0m] - Returning partial function: hyfi.utils.datasets.basic.DSBasic.dataframe_select_columns with kwargs: {'_target_': 'hyfi.utils.datasets.basic.DSBasic.dataframe_select_columns', 'columns': ['id', 'topic_relevant'], 'verbose': True}\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:16:40,802\u001b[0m][\u001b[34mhyfi.composer.composer\u001b[0m][\u001b[32mINFO\u001b[0m] - instantiating hyfi.utils.datasets.basic.DSBasic.dataframe_select_columns ...\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:16:40,803\u001b[0m][\u001b[34mhyfi.utils.datasets.basic\u001b[0m][\u001b[32mINFO\u001b[0m] - Selecting columns: ['id', 'topic_relevant']\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:16:40,808\u001b[0m][\u001b[34mhyfi.pipeline.config\u001b[0m][\u001b[32mINFO\u001b[0m] - Returning partial function: hyfi.utils.datasets.combine.DSCombine.merge_dataframes with kwargs: {'_target_': 'hyfi.utils.datasets.combine.DSCombine.merge_dataframes', 'right': 'datasets/processed/khmer_tokenized/train.parquet', 'how': 'inner', 'on': 'id', 'left_on': None, 'right_on': None, 'left_index': False, 'right_index': False, 'sort': False, 'suffixes': ['_x', '_y'], 'copy': True, 'indicator': False, 'validate': None, 'verbose': True}\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:16:40,809\u001b[0m][\u001b[34mhyfi.composer.composer\u001b[0m][\u001b[32mINFO\u001b[0m] - instantiating hyfi.utils.datasets.combine.DSCombine.merge_dataframes ...\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:16:40,812\u001b[0m][\u001b[34mhyfi.utils.datasets.combine\u001b[0m][\u001b[32mINFO\u001b[0m] - Merging dataframes\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:16:44,247\u001b[0m][\u001b[34mhyfi.pipeline.config\u001b[0m][\u001b[32mINFO\u001b[0m] - Running a pipe with hyfi.pipe.general_external_funcs\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:16:44,248\u001b[0m][\u001b[34mhyfi.pipeline.config\u001b[0m][\u001b[32mINFO\u001b[0m] - Returning partial function: hyfi.utils.datasets.basic.DSBasic.dataframe_print_head_and_tail with kwargs: {'_target_': 'hyfi.utils.datasets.basic.DSBasic.dataframe_print_head_and_tail', 'num_heads': 5, 'num_tails': 5, 'columns': ['id', 'text', 'tokens', 'topic_relevant'], 'verbose': True}\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:16:44,249\u001b[0m][\u001b[34mhyfi.composer.composer\u001b[0m][\u001b[32mINFO\u001b[0m] - instantiating hyfi.utils.datasets.basic.DSBasic.dataframe_print_head_and_tail ...\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:16:44,251\u001b[0m][\u001b[34mhyfi.utils.datasets.basic\u001b[0m][\u001b[32mINFO\u001b[0m] - Printing head and tail of dataframe\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:16:44,251\u001b[0m][\u001b[34mhyfi.utils.datasets.basic\u001b[0m][\u001b[32mINFO\u001b[0m] - Head:\u001b[0m\n",
      "          id  ... topic_relevant\n",
      "0  501330943  ...       0.341887\n",
      "1  501326169  ...       0.001473\n",
      "2  501325554  ...       0.000277\n",
      "3  501322478  ...       0.000366\n",
      "4  501321914  ...       0.999255\n",
      "\n",
      "[5 rows x 4 columns]\n",
      "[\u001b[36m2023-08-15 19:16:44,260\u001b[0m][\u001b[34mhyfi.utils.datasets.basic\u001b[0m][\u001b[32mINFO\u001b[0m] - Tail:\u001b[0m\n",
      "          id  ... topic_relevant\n",
      "27605  48239  ...       0.000566\n",
      "27606  48216  ...       0.000515\n",
      "27607  48132  ...       0.000197\n",
      "27608  48115  ...       0.572823\n",
      "27609  48085  ...       0.000182\n",
      "\n",
      "[5 rows x 4 columns]\n",
      "[\u001b[36m2023-08-15 19:16:44,272\u001b[0m][\u001b[34mhyfi.pipeline.config\u001b[0m][\u001b[32mINFO\u001b[0m] - Returning partial function: hyfi.utils.datasets.slice.DSSlice.filter_and_sample_data with kwargs: {'_target_': 'hyfi.utils.datasets.slice.DSSlice.filter_and_sample_data', 'queries': ['topic_relevant > 0.5'], 'sample_size': None, 'sample_seed': 42, 'output_dir': 'datasets/processed/topic_uncertainty_filtered', 'sample_filename': None, 'train_filename': 'train.parquet', 'discard_filename': 'discard.parquet', 'returning_data': 'train', 'verbose': True}\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:16:44,273\u001b[0m][\u001b[34mhyfi.composer.composer\u001b[0m][\u001b[32mINFO\u001b[0m] - instantiating hyfi.utils.datasets.slice.DSSlice.filter_and_sample_data ...\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:16:44,288\u001b[0m][\u001b[34mhyfi.utils.datasets.slice\u001b[0m][\u001b[32mINFO\u001b[0m] - filtering data by topic_relevant > 0.5\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:16:44,292\u001b[0m][\u001b[34mhyfi.utils.datasets.slice\u001b[0m][\u001b[32mINFO\u001b[0m] - filtered 20693 documents\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:16:44,295\u001b[0m][\u001b[34mhyfi.utils.datasets.save\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving dataframe to /raid/cis/yjlee/workspace/projects/nbcpu/workspace/datasets/processed/topic_uncertainty_filtered/train.parquet\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:16:48,417\u001b[0m][\u001b[34mhyfi.utils.datasets.save\u001b[0m][\u001b[32mINFO\u001b[0m] -  >> elapsed time to save data: 0:00:04.121659\u001b[0m\n",
      "           id  ...                                         predicates\n",
      "4   501321914  ...  [average, supply, drop, significantly, may, lo...\n",
      "5   501320777  ...  [see, significant, mobile, financial, introduc...\n",
      "10  501272597  ...  [french, national, nbc, establish, gain, print...\n",
      "11  501258270  ...  [french, national, nbc, establish, gain, print...\n",
      "12  501092971  ...  [european, central, ecb, become, great, could,...\n",
      "\n",
      "[5 rows x 12 columns]\n",
      "[\u001b[36m2023-08-15 19:16:48,438\u001b[0m][\u001b[34mhyfi.utils.datasets.save\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving dataframe to /raid/cis/yjlee/workspace/projects/nbcpu/workspace/datasets/processed/topic_uncertainty_filtered/discard.parquet\u001b[0m\n",
      "[\u001b[36m2023-08-15 19:17:08,321\u001b[0m][\u001b[34mhyfi.utils.datasets.save\u001b[0m][\u001b[32mINFO\u001b[0m] -  >> elapsed time to save data: 0:00:19.882527\u001b[0m\n",
      "          id  ...                                         predicates\n",
      "0  501330943  ...  [national, ask, financial, consider, provide, ...\n",
      "1  501326169  ...  [national, prepare, standardised, common, fina...\n",
      "2  501325554  ...  [national, prepare, standardised, common, fina...\n",
      "3  501322478  ...  [national, nbc, sign, chinese, international, ...\n",
      "6  501319461  ...  [national, launch, share, enable, financial, m...\n",
      "\n",
      "[5 rows x 12 columns]\n",
      "[\u001b[36m2023-08-15 19:17:08,338\u001b[0m][\u001b[34mhyfi.utils.datasets.slice\u001b[0m][\u001b[32mINFO\u001b[0m] - Created 0 samples, 6917 train samples, and 20693 discard samples\u001b[0m\n",
      " Change directory back to /raid/cis/yjlee/workspace/logs/hydra/nbcpu/2023-08-15/2023-08-15_19-16-37\n",
      "\u001b[0m[\u001b[36m2023-08-15 19:17:08,554\u001b[0m][\u001b[34mhyfi.task.task\u001b[0m][\u001b[32mINFO\u001b[0m] -  >> elapsed time for the task with 1 pipelines: 0:00:27.803055\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!nbcpu +workflow=nbcpu tasks='[nbcpu-datasets_uncertainty_filter]' mode=__info__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research-P5qh1W2b-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
